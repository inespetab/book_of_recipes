{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4cc23bb-7ef6-4129-9fd3-c397e3c3838d",
      "metadata": {
        "id": "e4cc23bb-7ef6-4129-9fd3-c397e3c3838d"
      },
      "source": [
        "### Hackathon 3: Optimisation of a Bioprocess with Multifidelity Bayesian Optimisation\n",
        "\n",
        "\n",
        "#### Hackathon Breif\n",
        "This hackathon involves the optimisation of a simulated bioprocess at process scale involving CHO cells to produce a desired protein. (ie. growing and feeding cells under precise conditions to produce the desired product).\n",
        "\n",
        "#### Inputs and Outputs\n",
        "Inputs to the bioprocess includes 5 vairables: the temperature [°C], pH and the concentration of feed [mM] at 3 different timepoints over 150 minutes. The output is the concentration of the titre (desired product) [g/L]. The goal is to obtain the input variables that correspond to the highest obtained titre.\n",
        "\n",
        "The bounds of the inputs are as follows:\n",
        "\n",
        "```\n",
        "temperature [°C]               -> 30 - 40\n",
        "pH                             -> 6 - 8\n",
        "first feed concentration [mM]  -> 0 - 50\n",
        "second feed concentration [mM] -> 0 - 50\n",
        "third feed concentration [mM]  -> 0 - 50\n",
        "```\n",
        "\n",
        "#### Fidelities and Running the simulation\n",
        "The simulations can be perfomed at 3 levels of fidelities with an associated accuracy and costs. These fidelities corresponds to a different reactor type and scale used.\n",
        "\n",
        "```\n",
        "Lowest fideility: 3L reactor with 1 feeding timepoint at 60 mins.\n",
        "Realtive cost: 10\n",
        "Remarks: The feeding concentration is taken as the second feed concentration. Lowest accuracy, but also lowest cost.\n",
        "\n",
        "Middle fidelity: 3L reactor with 3 feeding timepoints at 40, 80, 120 mins.\n",
        "Relative cost: 575\n",
        "Remarks: -\n",
        "\n",
        "Highest fidelity: 15L reactor with 3 feeding timepoints at 40, 80, 120 mins.\n",
        "Relative cost: 2100\n",
        "Remarks: Highest accuracy but high cost.\n",
        "```\n",
        "\n",
        "To run an experiment, one can use the `vl.conduct_experiment(X)` function -> this is your objective function. The inputs to this function is a matrix of shape (N, 6) where N is the number of data points and 6 refers to the total number of variables in the following order: `[temperature, pH, feed1, feed2, feed3, fidelity]`. The fidelities are refered to as integers where `0` corresponds to the lowest fidelity, `1` with the middle and `2` with the highest fidelity. An example is shown below.\n",
        "\n",
        "``` python\n",
        "def obj_func(X):\n",
        "\treturn -np.array(vl.conduct_experiment(X)) #negative placed if optimisation performed is minimisation\n",
        "\n",
        "X_initial = np.array([[33, 6.25, 10, 20, 20, 0],\n",
        "                      [38, 8, 20, 10, 20, 0]])\n",
        "Y_initial = vl.conduct_experiment(X_initial)\n",
        "print(Y_initial)\n",
        "```\n",
        "\n",
        "#### Goal and Submission\n",
        "Your goal is to develop a Bayesian Optimisation class to obtain the set of inputs which maximizes the titre. You have a budget of 10000 (observe the cost of running each fidelity), a maximum runtime (on the intructor's computer - be aware of how large the search space becomes especially with 6 dimensions!) and starting with a maximum of 6 training points. (Remember, you have to have at least 2 points for each variable for the covariance matrix to be calculated.)\n",
        "\n",
        "Like in previous hackathons, please submit your BO class (and GP class) along with the execution block to the Stremlit app. A different cell type (with different simulation parameters and maxima) will be used for scoring.\n",
        "\n",
        "This hackathon will be scored based on the sum of the normalised maximum titre concentration obtained.\n",
        "\n",
        "You must stay within the allocated budget! This will be checked, and if exceeded, your submission will be disqualified!\n",
        "\n",
        "#### Form of the BO class and execution block\n",
        "You are allowed to write your own BO class or make modifications to any of the previously seen BO classes.\n",
        "\n",
        "You must include the attributes `self.X` and `self.Y` corresponding to all of your evaluated inputs and outputs as this will be used to retrive the information used for scoring.\n",
        "\n",
        "```python\n",
        "#submission should look something like the following\n",
        "class GP: #if you have any separate classes other than the BO class\n",
        "    def __init__(self, ...):\n",
        "        ...\n",
        "#BO class\n",
        "class BO:\n",
        "    def __init__(self, ...):\n",
        "        self.X = #training data which the evaluated data is to be appended\n",
        "        self.Y = #evaluated via the objective function using self.X\n",
        "\n",
        "# BO Execution Block\n",
        "X_training = [...]\n",
        "X_seachspace = [...]\n",
        "\n",
        "BO_m = BO(...)\n",
        "```\n",
        "\n",
        "#### Guidance (Advanced)\n",
        "You must develop a multifidelity Bayesian Optimisation algorithm. Your scoring will be additionally penalised by the code runtime in the units of seconds. Make your algorithm as fast as it can go!\n",
        "\n",
        "\n",
        "#### Guidance (Intermediate)\n",
        "It is not mandatory for you to develop a multifidelity BO algorithm. You could, if you choose, use the single or batch BO algorithm developed previously to perform the optimisation. The lowest fidelity experiments do not offer accurate outcomes and you have to choose how many number of expeirments for each fidelity to be performed such that you do not exceed your allocated budget.\n",
        "\n",
        "However, if you do wish to tackle the hackathon via a multifidelity BO by modifying the single batch BO code from the first hackathon. Here are some pointers.\n",
        "1. Observe the output of the `vl.conduct_experiment(X)` function. The output does not have the same array structure/shape as the outputs obtained in the previous sections. You have to modify this in order to accomodate for the BO algorithm.\n",
        "2. Create a new acquisition function that is cost aware. We have previously used Lower Confidence Bound to balance exploration and exploitation of the search space. To make this cost aware, we can scale the values obtained from LCB by the cost.\n",
        "\n",
        "```python\n",
        "    def MF_lower_confidence_bound(...):\n",
        "        lower_std = Ysearchspace_mean - acquisition_hyperparam[0]*np.sqrt(Ysearchspace_std)\n",
        "        # mf_lower_std = lower_std / assocated cost for each simulation\n",
        "        return (X_searchspace[np.argmin(mf_lower_std)])\n",
        "```\n",
        "\n",
        "If this is done succefully, well done! However, you might see that the code will run rather slowly for each iteration (remember how the runtime scales with respect to additional dimensions in the search space). If you are finding it difficult to run the full iterations, a recommendation is to lower the number of total points in your search space. For example, if you are using the np.linspace() function, start with a very course number of points for each dimension (ex. 3) to develop your code. Once you are happy that the code can run without errors, then you can increase the number of points per dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd73484-4185-4126-8d42-fd3b5258cc61",
      "metadata": {
        "id": "4bd73484-4185-4126-8d42-fd3b5258cc61"
      },
      "source": [
        "#### Package Imports\n",
        "\n",
        "Packages are limited to the the ones listed in the package cell - Talk to one of the intructors to ask if it is possible to import other packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "efc60248-a788-4874-9922-36c336a69fcd",
      "metadata": {
        "id": "efc60248-a788-4874-9922-36c336a69fcd",
        "outputId": "c5d85dda-6819-4ea1-b0af-0a4546752e1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sobol_seq\n",
            "  Downloading sobol_seq-0.2.0-py3-none-any.whl.metadata (273 bytes)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sobol_seq) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sobol_seq) (2.0.2)\n",
            "Downloading sobol_seq-0.2.0-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: sobol_seq\n",
            "Successfully installed sobol_seq-0.2.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.15.3)\n",
            "Collecting linear-operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from linear-operator>=0.6->gpytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->linear-operator>=0.6->gpytorch) (3.0.2)\n",
            "Downloading gpytorch-1.14-py3-none-any.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, nvidia-cusolver-cu12, linear-operator, gpytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gpytorch-1.14 jaxtyping-0.3.2 linear-operator-0.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 wadler-lindig-0.1.7\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n",
            "Downloading rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl (34.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.3.3\n"
          ]
        }
      ],
      "source": [
        "# if using google collab, run the following pip installs!\n",
        "!pip install sobol_seq\n",
        "!pip install plotly\n",
        "!pip install gpytorch\n",
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "977ab0af-7637-40a7-ab13-e7c3491d0477",
      "metadata": {
        "id": "977ab0af-7637-40a7-ab13-e7c3491d0477"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
        "import plotly.graph_objs as go\n",
        "from scipy.integrate import quad\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.optimize import minimize, differential_evolution, NonlinearConstraint\n",
        "from sklearn.decomposition import PCA\n",
        "import math\n",
        "import time\n",
        "import sobol_seq\n",
        "import torch\n",
        "import gpytorch\n",
        "import copy\n",
        "\n",
        "import virtual_lab as vl\n",
        "import conditions_data as data\n",
        "from utils import standardize_data, unstandardize_y, train_gp_model, \\\n",
        "    expected_improvement, summed_feeding, optimize_acquisition_function, plot_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize, differential_evolution\n",
        "from scipy.stats import norm\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GP:\n",
        "    def __init__(self, kernel=None, alpha=1e-6, n_restarts_optimizer=5):\n",
        "        if kernel is None:\n",
        "            kernel = ConstantKernel(1.0) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel(noise_level=1e-5)\n",
        "\n",
        "        self.gp = GaussianProcessRegressor(\n",
        "            kernel=kernel,\n",
        "            alpha=alpha,\n",
        "            n_restarts_optimizer=n_restarts_optimizer,\n",
        "            normalize_y=True\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.gp.fit(X, y)\n",
        "\n",
        "    def predict(self, X, return_std=True):\n",
        "        return self.gp.predict(X, return_std=return_std)\n",
        "\n",
        "class BO:\n",
        "    def __init__(self, bounds, budget=10000, max_iter=100, xi=0.01,\n",
        "                 batch_strategy='constant_liar', batch_size=3, local_penalty_radius=0.1):\n",
        "        \"\"\"\n",
        "        Multifidelity Bayesian Optimization for CHO cell bioprocess with batch strategies\n",
        "\n",
        "        Args:\n",
        "            bounds: list of tuples for variable bounds [(min, max), ...]\n",
        "            budget: total cost budget\n",
        "            max_iter: maximum iterations\n",
        "            xi: exploration parameter for acquisition function\n",
        "            batch_strategy: 'constant_liar', 'kriging_believer', 'pessimistic_believer', 'local_penalization'\n",
        "            batch_size: number of points to select in each batch\n",
        "            local_penalty_radius: radius for local penalization\n",
        "        \"\"\"\n",
        "        self.bounds = np.array(bounds)\n",
        "        self.budget = budget\n",
        "        self.max_iter = max_iter\n",
        "        self.xi = xi\n",
        "        self.batch_strategy = batch_strategy\n",
        "        self.batch_size = batch_size\n",
        "        self.local_penalty_radius = local_penalty_radius\n",
        "\n",
        "        # Fidelity costs\n",
        "        self.fidelity_costs = {0: 10, 1: 575, 2: 2100}\n",
        "\n",
        "        # Storage for all evaluated points\n",
        "        self.X = np.empty((0, 6))  # [temp, pH, feed1, feed2, feed3, fidelity]\n",
        "        self.Y = np.empty((0,))\n",
        "\n",
        "        # Separate GPs for each fidelity level\n",
        "        self.gps = {0: GP(), 1: GP(), 2: GP()}\n",
        "\n",
        "        # Budget tracking\n",
        "        self.spent_budget = 0\n",
        "        self.iteration = 0\n",
        "\n",
        "        # For batch strategies - temporary data\n",
        "        self.temp_X = None\n",
        "        self.temp_Y = None\n",
        "        self.temp_gps = None\n",
        "\n",
        "        print(f\"Initialized BO with {batch_strategy} strategy, batch size: {batch_size}\")\n",
        "\n",
        "    def obj_func(self, X):\n",
        "        \"\"\"Objective function wrapper - returns negative for maximization\"\"\"\n",
        "        return -np.array(vl.conduct_experiment(X))\n",
        "\n",
        "    def add_observations(self, X_new, Y_new):\n",
        "        \"\"\"Add new observations to the dataset\"\"\"\n",
        "        if X_new.ndim == 1:\n",
        "            X_new = X_new.reshape(1, -1)\n",
        "        if Y_new.ndim == 0:\n",
        "            Y_new = np.array([Y_new])\n",
        "\n",
        "        self.X = np.vstack([self.X, X_new])\n",
        "        self.Y = np.concatenate([self.Y, Y_new])\n",
        "\n",
        "        # Update budget\n",
        "        for x in X_new:\n",
        "            fidelity = int(x[5])\n",
        "            self.spent_budget += self.fidelity_costs[fidelity]\n",
        "\n",
        "    def fit_gps(self):\n",
        "        \"\"\"Fit separate GPs for each fidelity level\"\"\"\n",
        "        for fidelity in [0, 1, 2]:\n",
        "            # Get data for this fidelity\n",
        "            fidelity_mask = self.X[:, 5] == fidelity\n",
        "            if np.sum(fidelity_mask) >= 2:  # Need at least 2 points\n",
        "                X_fid = self.X[fidelity_mask, :5]  # Exclude fidelity column\n",
        "                Y_fid = self.Y[fidelity_mask]\n",
        "                self.gps[fidelity].fit(X_fid, Y_fid)\n",
        "\n",
        "    def fit_temp_gps(self):\n",
        "        \"\"\"Fit GPs with temporary data for batch strategies\"\"\"\n",
        "        if self.temp_X is None:\n",
        "            return\n",
        "\n",
        "        self.temp_gps = {0: GP(), 1: GP(), 2: GP()}\n",
        "\n",
        "        for fidelity in [0, 1, 2]:\n",
        "            fidelity_mask = self.temp_X[:, 5] == fidelity\n",
        "            if np.sum(fidelity_mask) >= 2:\n",
        "                X_fid = self.temp_X[fidelity_mask, :5]\n",
        "                Y_fid = self.temp_Y[fidelity_mask]\n",
        "                self.temp_gps[fidelity].fit(X_fid, Y_fid)\n",
        "\n",
        "    def predict_with_correlation(self, X_vars, fidelity, use_temp=False):\n",
        "        \"\"\"\n",
        "        Predict using multifidelity correlation\n",
        "        Uses lower fidelity data to inform higher fidelity predictions\n",
        "        \"\"\"\n",
        "        gps_to_use = self.temp_gps if use_temp and self.temp_gps is not None else self.gps\n",
        "\n",
        "        if fidelity in gps_to_use and hasattr(gps_to_use[fidelity].gp, 'X_train_'):\n",
        "            # Direct prediction from same fidelity\n",
        "            mean, std = gps_to_use[fidelity].predict(X_vars)\n",
        "        else:\n",
        "            # Use lower fidelity as prior\n",
        "            if fidelity > 0 and hasattr(gps_to_use[fidelity-1].gp, 'X_train_'):\n",
        "                mean, std = gps_to_use[fidelity-1].predict(X_vars)\n",
        "                # Add uncertainty for fidelity difference\n",
        "                std = std * (1.5 if fidelity == 1 else 2.0)\n",
        "            else:\n",
        "                # Fallback to wide prior\n",
        "                mean = np.zeros(X_vars.shape[0])\n",
        "                std = np.ones(X_vars.shape[0]) * 10\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "    def acquisition_function(self, x, fidelity, exclude_points=None, use_temp=False):\n",
        "        \"\"\"\n",
        "        Acquisition function for a single point\n",
        "        \"\"\"\n",
        "        if x.ndim == 1:\n",
        "            x = x.reshape(1, -1)\n",
        "\n",
        "        # Get predictions\n",
        "        mean, std = self.predict_with_correlation(x, fidelity, use_temp)\n",
        "\n",
        "        # Lower Confidence Bound (for minimization)\n",
        "        lcb = mean - self.xi * std\n",
        "\n",
        "        # Cost-aware scaling\n",
        "        cost = self.fidelity_costs[fidelity]\n",
        "        cost_aware_lcb = lcb / np.sqrt(cost)\n",
        "\n",
        "        # Apply local penalization if needed\n",
        "        if exclude_points is not None and len(exclude_points) > 0 and self.batch_strategy == 'local_penalization':\n",
        "            for exclude_point in exclude_points:\n",
        "                distance = np.linalg.norm(x[0] - exclude_point[:5])\n",
        "                penalty = np.exp(-distance / self.local_penalty_radius)\n",
        "                cost_aware_lcb += penalty\n",
        "\n",
        "        return cost_aware_lcb[0]\n",
        "\n",
        "    def optimize_acquisition_for_fidelity(self, fidelity, exclude_points=None, use_temp=False):\n",
        "        \"\"\"\n",
        "        Optimize acquisition function for a specific fidelity level\n",
        "        \"\"\"\n",
        "        def objective(x):\n",
        "            return self.acquisition_function(x, fidelity, exclude_points, use_temp)\n",
        "\n",
        "        # Multiple random starts for global optimization\n",
        "        n_starts = 10\n",
        "        best_result = None\n",
        "        best_value = np.inf\n",
        "\n",
        "        for _ in range(n_starts):\n",
        "            # Random starting point\n",
        "            x0 = np.random.uniform(self.bounds[:, 0], self.bounds[:, 1])\n",
        "\n",
        "            try:\n",
        "                result = minimize(objective, x0, bounds=self.bounds, method='L-BFGS-B')\n",
        "                if result.success and result.fun < best_value:\n",
        "                    best_value = result.fun\n",
        "                    best_result = result\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Also try differential evolution for global search\n",
        "        try:\n",
        "            de_result = differential_evolution(objective, self.bounds, seed=42, maxiter=50)\n",
        "            if de_result.success and de_result.fun < best_value:\n",
        "                best_result = de_result\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        if best_result is None:\n",
        "            # Fallback to random point\n",
        "            x_best = np.random.uniform(self.bounds[:, 0], self.bounds[:, 1])\n",
        "        else:\n",
        "            x_best = best_result.x\n",
        "\n",
        "        # Add fidelity to the point\n",
        "        x_full = np.concatenate([x_best, [fidelity]])\n",
        "        cost = self.fidelity_costs[fidelity]\n",
        "\n",
        "        return x_full, cost\n",
        "\n",
        "    def find_best_point(self, exclude_points=None, use_temp=False):\n",
        "        \"\"\"\n",
        "        Find the best point across all fidelities\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        for fidelity in [0, 1, 2]:\n",
        "            try:\n",
        "                x_best, cost = self.optimize_acquisition_for_fidelity(fidelity, exclude_points, use_temp)\n",
        "                acq_value = self.acquisition_function(x_best[:5], fidelity, exclude_points, use_temp)\n",
        "                candidates.append((acq_value, x_best, cost, fidelity))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if not candidates:\n",
        "            # Emergency fallback\n",
        "            x_rand = np.random.uniform(self.bounds[:, 0], self.bounds[:, 1])\n",
        "            fidelity = 0  # Use cheapest fidelity\n",
        "            x_full = np.concatenate([x_rand, [fidelity]])\n",
        "            cost = self.fidelity_costs[fidelity]\n",
        "            print(\"Emergency fallback\")\n",
        "            return x_full, cost\n",
        "\n",
        "        # Select best candidate\n",
        "        best_candidate = min(candidates, key=lambda x: x[0])\n",
        "        return best_candidate[1], best_candidate[2]\n",
        "\n",
        "    def constant_liar_batch(self):\n",
        "        \"\"\"Constant Liar batch selection strategy\"\"\"\n",
        "        batch_points = []\n",
        "\n",
        "        # Initialize temporary data\n",
        "        self.temp_X = self.X.copy()\n",
        "        self.temp_Y = self.Y.copy()\n",
        "\n",
        "        # Use current best as the \"lie\" value\n",
        "        lie_value = np.min(self.Y) if len(self.Y) > 0 else 0.0\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # Fit temporary GPs\n",
        "            self.fit_temp_gps()\n",
        "\n",
        "            # Find best point using temporary GPs\n",
        "            best_x, cost = self.find_best_point(use_temp=True)\n",
        "\n",
        "            # Check budget\n",
        "            if self.spent_budget + sum([p[1] for p in batch_points]) + cost > self.budget:\n",
        "                break\n",
        "\n",
        "            batch_points.append((best_x, cost))\n",
        "\n",
        "            # Add \"lie\" observation to temporary dataset\n",
        "            self.temp_X = np.vstack([self.temp_X, best_x])\n",
        "            self.temp_Y = np.concatenate([self.temp_Y, [lie_value]])\n",
        "\n",
        "        # Clear temporary data\n",
        "        self.temp_X = None\n",
        "        self.temp_Y = None\n",
        "        self.temp_gps = None\n",
        "\n",
        "        return batch_points\n",
        "\n",
        "    def kriging_believer_batch(self):\n",
        "        \"\"\"Kriging Believer batch selection strategy\"\"\"\n",
        "        batch_points = []\n",
        "\n",
        "        # Initialize temporary data\n",
        "        self.temp_X = self.X.copy()\n",
        "        self.temp_Y = self.Y.copy()\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # Fit temporary GPs\n",
        "            self.fit_temp_gps()\n",
        "\n",
        "            # Find best point using temporary GPs\n",
        "            best_x, cost = self.find_best_point(use_temp=True)\n",
        "\n",
        "            # Check budget\n",
        "            if self.spent_budget + sum([p[1] for p in batch_points]) + cost > self.budget:\n",
        "                break\n",
        "\n",
        "            # Get predicted value (kriging believer)\n",
        "            fidelity = int(best_x[5])\n",
        "            mean, _ = self.predict_with_correlation(best_x[:5].reshape(1, -1), fidelity, use_temp=True)\n",
        "            predicted_value = mean[0]\n",
        "\n",
        "            batch_points.append((best_x, cost))\n",
        "\n",
        "            # Add predicted observation to temporary dataset\n",
        "            self.temp_X = np.vstack([self.temp_X, best_x])\n",
        "            self.temp_Y = np.concatenate([self.temp_Y, [predicted_value]])\n",
        "\n",
        "        # Clear temporary data\n",
        "        self.temp_X = None\n",
        "        self.temp_Y = None\n",
        "        self.temp_gps = None\n",
        "\n",
        "        return batch_points\n",
        "\n",
        "    def pessimistic_believer_batch(self):\n",
        "        \"\"\"Pessimistic Believer batch selection strategy\"\"\"\n",
        "        batch_points = []\n",
        "\n",
        "        # Initialize temporary data\n",
        "        self.temp_X = self.X.copy()\n",
        "        self.temp_Y = self.Y.copy()\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # Fit temporary GPs\n",
        "            self.fit_temp_gps()\n",
        "\n",
        "            # Find best point using temporary GPs\n",
        "            best_x, cost = self.find_best_point(use_temp=True)\n",
        "\n",
        "            # Check budget\n",
        "            if self.spent_budget + sum([p[1] for p in batch_points]) + cost > self.budget:\n",
        "                break\n",
        "\n",
        "            # Get pessimistic prediction (mean + std for minimization)\n",
        "            fidelity = int(best_x[5])\n",
        "            mean, std = self.predict_with_correlation(best_x[:5].reshape(1, -1), fidelity, use_temp=True)\n",
        "            pessimistic_value = mean[0] + std[0]  # Add std for minimization (worse value)\n",
        "\n",
        "            batch_points.append((best_x, cost))\n",
        "\n",
        "            # Add pessimistic observation to temporary dataset\n",
        "            self.temp_X = np.vstack([self.temp_X, best_x])\n",
        "            self.temp_Y = np.concatenate([self.temp_Y, [pessimistic_value]])\n",
        "\n",
        "        # Clear temporary data\n",
        "        self.temp_X = None\n",
        "        self.temp_Y = None\n",
        "        self.temp_gps = None\n",
        "\n",
        "        return batch_points\n",
        "\n",
        "    def local_penalization_batch(self):\n",
        "        \"\"\"Local Penalization batch selection strategy\"\"\"\n",
        "        batch_points = []\n",
        "        selected_points = []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # Find best point with local penalization\n",
        "            best_x, cost = self.find_best_point(exclude_points=selected_points)\n",
        "\n",
        "            # Check budget\n",
        "            if self.spent_budget + sum([p[1] for p in batch_points]) + cost > self.budget:\n",
        "                break\n",
        "\n",
        "            batch_points.append((best_x, cost))\n",
        "            selected_points.append(best_x)\n",
        "\n",
        "        return batch_points\n",
        "\n",
        "    def select_batch(self):\n",
        "        \"\"\"Select batch of points using chosen strategy\"\"\"\n",
        "        # Apply selected batch strategy\n",
        "        if self.batch_strategy == 'constant_liar':\n",
        "            return self.constant_liar_batch()\n",
        "        elif self.batch_strategy == 'kriging_believer':\n",
        "            return self.kriging_believer_batch()\n",
        "        elif self.batch_strategy == 'pessimistic_believer':\n",
        "            return self.pessimistic_believer_batch()\n",
        "        elif self.batch_strategy == 'local_penalization':\n",
        "            return self.local_penalization_batch()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown batch strategy: {self.batch_strategy}\")\n",
        "\n",
        "    def initialize(self, n_initial=6):\n",
        "        \"\"\"Initialize with diverse points across fidelities\"\"\"\n",
        "        np.random.seed()\n",
        "\n",
        "        X_init = []\n",
        "        for i in range(n_initial):\n",
        "            x = []\n",
        "            for j in range(5):  # First 5 variables\n",
        "                x.append(np.random.uniform(self.bounds[j, 0], self.bounds[j, 1]))\n",
        "\n",
        "            # Assign fidelity (mostly low fidelity initially)\n",
        "            if i < 4:\n",
        "                x.append(0)  # Low fidelity\n",
        "            elif i < 5:\n",
        "                x.append(1)  # Medium fidelity\n",
        "            else:\n",
        "                x.append(2)  # High fidelity\n",
        "\n",
        "            X_init.append(x)\n",
        "\n",
        "        X_init = np.array(X_init)\n",
        "        Y_init = self.obj_func(X_init)\n",
        "\n",
        "        self.add_observations(X_init, Y_init)\n",
        "        self.fit_gps()\n",
        "\n",
        "    def optimize(self):\n",
        "        \"\"\"Main optimization loop with batch selection\"\"\"\n",
        "        print(f\"Starting optimization with budget: {self.budget}\")\n",
        "        print(f\"Using {self.batch_strategy} strategy with batch size: {self.batch_size}\")\n",
        "        print(f\"Initial budget spent: {self.spent_budget}\")\n",
        "\n",
        "        while (self.iteration < self.max_iter and\n",
        "               self.spent_budget < self.budget * 0.95):  # Leave 5% buffer\n",
        "\n",
        "            # Select batch of points\n",
        "            batch_points = self.select_batch()\n",
        "\n",
        "            if not batch_points:\n",
        "                print(\"No valid points found within budget. Stopping optimization.\")\n",
        "                break\n",
        "\n",
        "            # Evaluate batch\n",
        "            batch_X = np.array([point[0] for point in batch_points])\n",
        "            batch_costs = [point[1] for point in batch_points]\n",
        "\n",
        "            # Check total batch cost\n",
        "            total_batch_cost = sum(batch_costs)\n",
        "            if self.spent_budget + total_batch_cost > self.budget:\n",
        "                print(\"Batch would exceed budget. Stopping optimization.\")\n",
        "                break\n",
        "\n",
        "            # Evaluate all points in batch\n",
        "            Y_batch = self.obj_func(batch_X)\n",
        "            self.add_observations(batch_X, Y_batch)\n",
        "\n",
        "            # Refit GPs with new data\n",
        "            self.fit_gps()\n",
        "\n",
        "            self.iteration += 1\n",
        "\n",
        "            if self.iteration % 3 == 0:\n",
        "                current_best = np.max(-self.Y)  # Convert back to maximization\n",
        "                print(f\"Iteration {self.iteration}: Batch size: {len(batch_points)}, Batch cost: {total_batch_cost}, Budget spent: {self.spent_budget}/{self.budget}, Best titre: {current_best:.3f}\")\n",
        "\n",
        "            #print(batch_X)\n",
        "\n",
        "        # Final summary\n",
        "        best_idx = np.argmax(-self.Y)\n",
        "        best_x = self.X[best_idx]\n",
        "        best_y = -self.Y[best_idx]\n",
        "\n",
        "        print(f\"\\nOptimization completed!\")\n",
        "        print(f\"Strategy used: {self.batch_strategy}\")\n",
        "        print(f\"Total iterations: {self.iteration}\")\n",
        "        print(f\"Budget spent: {self.spent_budget}/{self.budget}\")\n",
        "        print(f\"Best titre concentration: {best_y:.4f} g/L\")\n",
        "        print(f\"Best parameters: Temp={best_x[0]:.1f}°C, pH={best_x[1]:.2f}, Feeds=[{best_x[2]:.1f}, {best_x[3]:.1f}, {best_x[4]:.1f}] mM, Fidelity={int(best_x[5])}\")\n",
        "\n",
        "# BO Execution Block\n",
        "if __name__ == \"__main__\":\n",
        "    # Define bounds for [temperature, pH, feed1, feed2, feed3]\n",
        "    bounds = [\n",
        "        (30, 40),    # temperature [°C]\n",
        "        (6, 8),      # pH\n",
        "        (0, 50),     # first feed concentration [mM]\n",
        "        (0, 50),     # second feed concentration [mM]\n",
        "        (0, 50)      # third feed concentration [mM]\n",
        "    ]\n",
        "\n",
        "    # Choose your batch strategy here:\n",
        "    # Options: 'constant_liar', 'kriging_believer', 'pessimistic_believer', 'local_penalization'\n",
        "    strategy = 'kriging_believer'  # Change this to try different strategies\n",
        "\n",
        "    # Initialize BO with chosen strategy\n",
        "    BO_m = BO(bounds=bounds, budget=10000, max_iter=35, xi=0.1,\n",
        "              batch_strategy=strategy, batch_size=6, local_penalty_radius=0.15)\n",
        "\n",
        "    # Initialize with starting points\n",
        "    BO_m.initialize(n_initial=6)\n",
        "\n",
        "    # Run optimization\n",
        "    BO_m.optimize()\n",
        "\n",
        "    # Results are stored in BO_m.X and BO_m.Y\n",
        "    print(f\"\\nFinal dataset size: {len(BO_m.X)} evaluations\")\n",
        "    print(f\"Maximum titre achieved: {np.max(-BO_m.Y):.4f} g/L\")\n",
        "\n",
        "    # Uncomment below to try different strategies in sequence\n",
        "\n",
        "    strategies = ['constant_liar', 'kriging_believer', 'pessimistic_believer', 'local_penalization']\n",
        "\n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Testing {strategy.upper()} strategy\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        BO_test = BO(bounds=bounds, budget=10000, max_iter=20, xi=0.1,\n",
        "                     batch_strategy=strategy, batch_size=3, local_penalty_radius=0.15)\n",
        "        BO_test.initialize(n_initial=6)\n",
        "        BO_test.optimize()\n",
        "\n",
        "        print(f\"Final result for {strategy}: {np.max(-BO_test.Y):.4f} g/L\")\n",
        ""
      ],
      "metadata": {
        "id": "k0TQqLqhTkL5",
        "outputId": "0b9dbc31-748e-44cb-8879-52796c214766",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "k0TQqLqhTkL5",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized BO with kriging_believer strategy, batch size: 6\n",
            "Starting optimization with budget: 10000\n",
            "Using kriging_believer strategy with batch size: 6\n",
            "Initial budget spent: 2715\n",
            "Iteration 3: Batch size: 6, Batch cost: 60, Budget spent: 2895/10000, Best titre: 27.685\n",
            "Iteration 6: Batch size: 6, Batch cost: 60, Budget spent: 3075/10000, Best titre: 27.685\n",
            "Iteration 9: Batch size: 6, Batch cost: 60, Budget spent: 3255/10000, Best titre: 72.254\n",
            "Iteration 12: Batch size: 6, Batch cost: 60, Budget spent: 3435/10000, Best titre: 73.513\n",
            "Iteration 15: Batch size: 6, Batch cost: 60, Budget spent: 3615/10000, Best titre: 73.513\n",
            "Iteration 18: Batch size: 6, Batch cost: 60, Budget spent: 3795/10000, Best titre: 73.513\n",
            "Iteration 21: Batch size: 6, Batch cost: 60, Budget spent: 3975/10000, Best titre: 73.513\n",
            "Iteration 24: Batch size: 6, Batch cost: 60, Budget spent: 4155/10000, Best titre: 73.513\n",
            "Iteration 27: Batch size: 6, Batch cost: 60, Budget spent: 4335/10000, Best titre: 73.513\n",
            "Iteration 30: Batch size: 6, Batch cost: 60, Budget spent: 4515/10000, Best titre: 73.513\n",
            "Iteration 33: Batch size: 6, Batch cost: 60, Budget spent: 4695/10000, Best titre: 73.513\n",
            "\n",
            "Optimization completed!\n",
            "Strategy used: kriging_believer\n",
            "Total iterations: 35\n",
            "Budget spent: 4815/10000\n",
            "Best titre concentration: 73.5135 g/L\n",
            "Best parameters: Temp=32.6°C, pH=6.68, Feeds=[8.7, 24.2, 31.6] mM, Fidelity=0\n",
            "\n",
            "Final dataset size: 216 evaluations\n",
            "Maximum titre achieved: 73.5135 g/L\n",
            "\n",
            "============================================================\n",
            "Testing CONSTANT_LIAR strategy\n",
            "============================================================\n",
            "Initialized BO with constant_liar strategy, batch size: 3\n",
            "Starting optimization with budget: 10000\n",
            "Using constant_liar strategy with batch size: 3\n",
            "Initial budget spent: 2715\n",
            "Iteration 3: Batch size: 3, Batch cost: 30, Budget spent: 2805/10000, Best titre: 56.404\n",
            "Iteration 6: Batch size: 3, Batch cost: 30, Budget spent: 2895/10000, Best titre: 56.404\n",
            "Iteration 9: Batch size: 3, Batch cost: 30, Budget spent: 2985/10000, Best titre: 56.404\n",
            "Iteration 12: Batch size: 3, Batch cost: 30, Budget spent: 3075/10000, Best titre: 56.404\n",
            "Iteration 15: Batch size: 3, Batch cost: 30, Budget spent: 3165/10000, Best titre: 56.404\n",
            "Iteration 18: Batch size: 3, Batch cost: 30, Budget spent: 3255/10000, Best titre: 56.404\n",
            "\n",
            "Optimization completed!\n",
            "Strategy used: constant_liar\n",
            "Total iterations: 20\n",
            "Budget spent: 3315/10000\n",
            "Best titre concentration: 56.4038 g/L\n",
            "Best parameters: Temp=30.2°C, pH=6.65, Feeds=[36.4, 43.7, 6.7] mM, Fidelity=1\n",
            "Final result for constant_liar: 56.4038 g/L\n",
            "\n",
            "============================================================\n",
            "Testing KRIGING_BELIEVER strategy\n",
            "============================================================\n",
            "Initialized BO with kriging_believer strategy, batch size: 3\n",
            "Starting optimization with budget: 10000\n",
            "Using kriging_believer strategy with batch size: 3\n",
            "Initial budget spent: 2715\n",
            "Iteration 3: Batch size: 3, Batch cost: 30, Budget spent: 2805/10000, Best titre: 19.900\n",
            "Iteration 6: Batch size: 3, Batch cost: 30, Budget spent: 2895/10000, Best titre: 19.900\n",
            "Iteration 9: Batch size: 3, Batch cost: 30, Budget spent: 2985/10000, Best titre: 35.629\n",
            "Iteration 12: Batch size: 3, Batch cost: 30, Budget spent: 3075/10000, Best titre: 35.629\n",
            "Iteration 15: Batch size: 3, Batch cost: 30, Budget spent: 3165/10000, Best titre: 35.629\n",
            "Iteration 18: Batch size: 3, Batch cost: 30, Budget spent: 3255/10000, Best titre: 35.629\n",
            "\n",
            "Optimization completed!\n",
            "Strategy used: kriging_believer\n",
            "Total iterations: 20\n",
            "Budget spent: 3315/10000\n",
            "Best titre concentration: 35.6287 g/L\n",
            "Best parameters: Temp=33.1°C, pH=7.63, Feeds=[30.0, 40.7, 2.9] mM, Fidelity=0\n",
            "Final result for kriging_believer: 35.6287 g/L\n",
            "\n",
            "============================================================\n",
            "Testing PESSIMISTIC_BELIEVER strategy\n",
            "============================================================\n",
            "Initialized BO with pessimistic_believer strategy, batch size: 3\n",
            "Starting optimization with budget: 10000\n",
            "Using pessimistic_believer strategy with batch size: 3\n",
            "Initial budget spent: 2715\n",
            "Iteration 3: Batch size: 3, Batch cost: 30, Budget spent: 2805/10000, Best titre: 43.809\n",
            "Iteration 6: Batch size: 3, Batch cost: 30, Budget spent: 2895/10000, Best titre: 43.809\n",
            "Iteration 9: Batch size: 3, Batch cost: 30, Budget spent: 2985/10000, Best titre: 43.809\n",
            "Iteration 12: Batch size: 3, Batch cost: 30, Budget spent: 3075/10000, Best titre: 43.809\n",
            "Iteration 15: Batch size: 3, Batch cost: 30, Budget spent: 3165/10000, Best titre: 43.809\n",
            "Iteration 18: Batch size: 3, Batch cost: 30, Budget spent: 3255/10000, Best titre: 43.809\n",
            "\n",
            "Optimization completed!\n",
            "Strategy used: pessimistic_believer\n",
            "Total iterations: 20\n",
            "Budget spent: 3315/10000\n",
            "Best titre concentration: 43.8091 g/L\n",
            "Best parameters: Temp=31.6°C, pH=6.43, Feeds=[31.0, 30.1, 40.7] mM, Fidelity=2\n",
            "Final result for pessimistic_believer: 43.8091 g/L\n",
            "\n",
            "============================================================\n",
            "Testing LOCAL_PENALIZATION strategy\n",
            "============================================================\n",
            "Initialized BO with local_penalization strategy, batch size: 3\n",
            "Starting optimization with budget: 10000\n",
            "Using local_penalization strategy with batch size: 3\n",
            "Initial budget spent: 2715\n",
            "Iteration 3: Batch size: 3, Batch cost: 30, Budget spent: 2805/10000, Best titre: 35.821\n",
            "Iteration 6: Batch size: 3, Batch cost: 30, Budget spent: 2895/10000, Best titre: 35.821\n",
            "Iteration 9: Batch size: 3, Batch cost: 30, Budget spent: 2985/10000, Best titre: 35.821\n",
            "Iteration 12: Batch size: 3, Batch cost: 30, Budget spent: 3075/10000, Best titre: 35.821\n",
            "Iteration 15: Batch size: 3, Batch cost: 30, Budget spent: 3165/10000, Best titre: 35.821\n",
            "Iteration 18: Batch size: 3, Batch cost: 30, Budget spent: 3255/10000, Best titre: 35.821\n",
            "\n",
            "Optimization completed!\n",
            "Strategy used: local_penalization\n",
            "Total iterations: 20\n",
            "Budget spent: 3315/10000\n",
            "Best titre concentration: 35.8206 g/L\n",
            "Best parameters: Temp=33.3°C, pH=7.20, Feeds=[2.6, 17.9, 37.0] mM, Fidelity=2\n",
            "Final result for local_penalization: 35.8206 g/L\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}