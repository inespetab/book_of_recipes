{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f9127b8-b43c-4b26-89fd-60b61ddf5ae4",
      "metadata": {
        "id": "1f9127b8-b43c-4b26-89fd-60b61ddf5ae4"
      },
      "source": [
        "## Bayesian Optimisation for Chemical Reaction Optimisation: Example Application of BO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e54b87-fa2b-426e-a1c4-0351211877a8",
      "metadata": {
        "id": "81e54b87-fa2b-426e-a1c4-0351211877a8"
      },
      "source": [
        "Well done for powering through the previous sections! Using our understanding of the basics of Bayesian Optimisation, we shall now observe an application of Bayesian Optimisation: Chemical Reaction Optimisation.\n",
        "\n",
        "Running a reaction condition optimisation campaign can be cumbersome as experimentation can be time consuming especially under the vast combinatorial domain of the input reaction condition search space (reagent identities, equivalences, solvent identities, reaction temperature etc.). Bayesian Optimisation could be a useful method to make better decisions (balance of exploration and exploitation conditioned on known expeirmental outcomes) when deciding on which experiments to run.\n",
        "\n",
        "Before tackling the following notebook, think about what changes to the workflow or code we have seen in previous section to allow Bayesian Optimisation to be a practical tool for experimental chemical optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9c61b7c-647e-481c-a987-ef8fcd1abb0d",
      "metadata": {
        "id": "d9c61b7c-647e-481c-a987-ef8fcd1abb0d"
      },
      "source": [
        "### Importing relevant packages\n",
        "(Feel free to import any packages you feel like needing to fully explore the content!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "77ff5212-d0cf-472d-bf93-d9329e8d5b70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ff5212-d0cf-472d-bf93-d9329e8d5b70",
        "outputId": "3b020cdc-d583-4381-bc08-18330ae133da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sobol_seq\n",
            "  Downloading sobol_seq-0.2.0-py3-none-any.whl.metadata (273 bytes)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sobol_seq) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sobol_seq) (2.0.2)\n",
            "Downloading sobol_seq-0.2.0-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: sobol_seq\n",
            "Successfully installed sobol_seq-0.2.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.15.3)\n",
            "Collecting linear-operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from linear-operator>=0.6->gpytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->linear-operator>=0.6->gpytorch) (3.0.2)\n",
            "Downloading gpytorch-1.14-py3-none-any.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, nvidia-cusolver-cu12, linear-operator, gpytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gpytorch-1.14 jaxtyping-0.3.2 linear-operator-0.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 wadler-lindig-0.1.7\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n",
            "Downloading rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl (34.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.3.3\n"
          ]
        }
      ],
      "source": [
        "# if using google collab, run the following pip installs!\n",
        "!pip install sobol_seq\n",
        "!pip install plotly\n",
        "!pip install gpytorch\n",
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "08bd5451-e9de-4933-a59f-0f7e24e19046",
      "metadata": {
        "id": "08bd5451-e9de-4933-a59f-0f7e24e19046"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "from scipy.integrate import quad\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.optimize import minimize\n",
        "import math\n",
        "import time\n",
        "import sobol_seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c9cf95-cfff-42b0-8139-d523c319de7e",
      "metadata": {
        "id": "25c9cf95-cfff-42b0-8139-d523c319de7e"
      },
      "source": [
        "## Section A: Introduction to Batch Bayesian Optimisation\n",
        "A useful modification to the BO workflow is to allow for the acquisition functions to provide multiple/batch recommedations as chemical experiments can be performed in parallel. (Context: If you are an experimental chemist, think how many expeiments you can dispense, react, isolate and analyse in a given period vs consecutive singleton reactions.)\n",
        "\n",
        "Although gaining speed from performing evaluations in parallel, the recommendations would be less informed, as batch recommendations are based on the conditioning of a smaller set of data when compared to singleton experiments. The suitable number of batch experiments is usually dependant on how many experiments can be run in parallel which is heavily reliant on the type of chemistry and the chemist who is runnning the experiment.\n",
        "\n",
        "In this section, we will explore batch Bayesian Optimisation with Thompson Sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a36e18e-5461-401c-867b-218d7a1f6fc4",
      "metadata": {
        "id": "2a36e18e-5461-401c-867b-218d7a1f6fc4"
      },
      "source": [
        "#### Excersise A1_1:\n",
        "##### Concepts: Run Time of Bayesian Optimisation\n",
        "\n",
        "Apart form being able to perform experiments in parallel, batch BO can help speed up the runtime of the BO code. Using the GP and BO classes you have defined previously in Day 1, determine how long each section of the code (GP regression, GP inference, acquisition function etc.) takes to run. you can use the time module to help.\n",
        "\n",
        "```python\n",
        "time1 = time.time()\n",
        "...\n",
        "time2 = time.time()\n",
        "print('Time taken in seconds: ', time2-time1)\n",
        "```\n",
        "\n",
        "Which section runs with constant time over each iteration? Which section takes longer time to run? Why is this? Compare the magnitude of the time taken for each section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f151f8-c0cb-4fe9-ba99-2c41cebb3035",
      "metadata": {
        "id": "51f151f8-c0cb-4fe9-ba99-2c41cebb3035"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4cd25ecc-b6e1-4e2b-8760-584d48ea1e3b",
      "metadata": {
        "id": "4cd25ecc-b6e1-4e2b-8760-584d48ea1e3b"
      },
      "source": [
        "#### Excersise A1_2:\n",
        "##### Concepts: Run Time of Bayesian Optimisation\n",
        "\n",
        "Now increase your input dimensions by 1 (use an arbitrary objective function that takes in 3 inputs and returns 1 output). Keep the number of training data and the number of point per dimension the same. How long does it take to run each section of the code? How the the time taken scale with the number of dimensions - why is this so?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77958001-5aa5-44f5-b784-a9e271eb44fd",
      "metadata": {
        "id": "77958001-5aa5-44f5-b784-a9e271eb44fd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c9fb8896-4e08-46a2-9a8c-3520ad05ed66",
      "metadata": {
        "id": "c9fb8896-4e08-46a2-9a8c-3520ad05ed66"
      },
      "source": [
        "#### Excersise A1_3:\n",
        "##### Concepts: Priors, Posteriors, Thompson Sampling\n",
        "\n",
        "You should have observed that the acquisition function takes the least time to run and in several orders of magnitude quicker than the GP regression and inference. It could be useful to perform more evaluations per round of BO if the GP regression/inference requires a longer time, especially with higher dimensions.\n",
        "\n",
        "We are going to observe an implimentation of batch BO using Thompson Sampling. But before this, we are going to first see GPs from a slightly different, but related angle. We have seen that the result of GP inference is that we obtain a mean and an associated variance of the search space is a result of conditioning on trainig data. Another view of the outcome of GP inference is that we obtain a **distribution of functions** that are conditioned on the trianing data of which gives the mean and associated variance over the interested search space.\n",
        "\n",
        "We have made assumptions about the mean and kernel functions as seen in previous sections. Prior to conditioning, samples of this distribution can be plotted (with a given set of kernel hyperparameters).\n",
        "\n",
        "![download.png](attachment:63cb9bc0-14b9-44fb-83d8-672dc148040a.png)\n",
        "\n",
        "We can see that samples of the distribution of functions averages to the described mean function (here, = 0) with a constant variance (variance is not plotted here) over the entire search space. These functions are called the priors.\n",
        "\n",
        "After conditioning, we can see the conditioned priors go through exactly the points of the training data (noise is not considered here). The average of these functions give the mean and provides the associated variance. The functions are called the posteriors.\n",
        "\n",
        "![download.png](attachment:67478bcf-2c7b-4a14-8b79-eed4f3c97f2c.png)\n",
        "\n",
        "We have seen previously that the acquisition function considered the mean and associated variance to obtain a single recommendation. Thompson Sampling allows us to obtain batch recommendations by considering individual posterior functions obtained from the distribution instead of the mean and variance directly. The number of recommendations (batch number) determines the number of sampled posteriors we require to obtain.\n",
        "\n",
        "To obtain the posteriors, we need to obtain the mean and full covariance matrix of all points of the interested search space as each point in this space will have to be conditioned on every other point in this space. A function for doing so is created and seen below. It takes in an object of the GP_model_zeromean class and the interested search space as inputs and gives the mean and covariance matrix as output.\n",
        "\n",
        "We can then draw posteriors using the `np.random.multivariate_normal()` function.\n",
        "\n",
        "``` python\n",
        "def GP_predict_full(gp, Xtest):\n",
        "    \"\"\"\n",
        "    Returns (mean_vector, cov_matrix) at the inputs Xstar_norm.\n",
        "    \"\"\"\n",
        "    Xstar_norm = (Xtest - gp.X_mean) / gp.X_std\n",
        "    # unpack\n",
        "    ell, sf2 = np.exp(2*gp.hyperparam_optimized[:gp.nx_dimensions, 0]), \\\n",
        "                np.exp(2*gp.hyperparam_optimized[gp.nx_dimensions, 0])\n",
        "    sn2 = np.exp(2*gp.hyperparam_optimized[gp.nx_dimensions+1, 0]) + 1e-8\n",
        "    \n",
        "    # training data\n",
        "    Xn, Yn = gp.X_norm, gp.Y_norm[:,0]         # here we only have one output\n",
        "    K = gp.Cov_mat(gp.kernel, Xn, ell, sf2) + sn2*np.eye(gp.number_of_point)\n",
        "    K = (K + K.T)*0.5\n",
        "    Kinv = np.linalg.solve(K, np.eye(K.shape[0]))\n",
        "\n",
        "    # cross-covariances\n",
        "    # shape: (n_train, n_test)\n",
        "    Ks = sf2 * np.exp(-0.5 * cdist(Xn, Xstar_norm, 'seuclidean', V=ell)**2)\n",
        "\n",
        "    # test-test covariance\n",
        "    Kss = sf2 * np.exp(-0.5 * cdist(Xstar_norm, Xstar_norm, 'seuclidean', V=ell)**2)\n",
        "\n",
        "    # predictive mean and covariance\n",
        "    mu_star = Ks.T.dot(Kinv).dot(Yn)\n",
        "    cov_star = Kss - Ks.T.dot(Kinv).dot(Ks)\n",
        "    # ensure symmetry\n",
        "    cov_star = (cov_star + cov_star.T)*0.5\n",
        "\n",
        "    #unnormalise the mean and covariance matrix\n",
        "    mu_plot = mu_star*gp.Y_std + gp.Y_mean\n",
        "    cov_plot = cov_star * (gp.Y_std**2)\n",
        "    return  mu_plot, cov_plot\n",
        "\n",
        "mu_plot, cov_plot = GP_predict_full(GP_m, X_searchspace.reshape(-1,1))\n",
        "samples = np.random.multivariate_normal(mu_plot.flatten(), cov_plot, size=50) #drawing 50 posterior samples\n",
        "plt.plot(X_searchspace.reshape(-1,1), samples[1], lw=0.5) #we can plot the posteriors, here the first of 50 posterior is plotted\n",
        "\n",
        "```\n",
        "\n",
        "Using this function, can you recreate the posterior plot as seen above? (We are not performing BO here, just GP regression, inference and obtaining posteriors.)\n",
        "\n",
        "```python\n",
        "def obj_func(x):\n",
        "\treturn np.sin(x**np.sin(x))\n",
        "\n",
        "# training data\n",
        "X_training = np.array([3.5, 4, 6, 7])\n",
        "ndata      = X_training.shape[0]\n",
        "X_training = X_training.reshape(ndata,1)\n",
        "Y_training = obj_func(X_training )\n",
        "\n",
        "# search space\n",
        "number_points_searchspace  = 500\n",
        "X_searchspace              = np.linspace(3, 8.0, num=number_points_searchspace)\n",
        "\n",
        "Ysearchspace_mean        = np.zeros(number_points_searchspace)\n",
        "Ysearchspace_std         = np.zeros(number_points_searchspace)\n",
        "\n",
        "GP_m = GP_model_meanzero(X_training, Y_training, 'SquaredExponential', hyperparams_multistart_loops = 3)\n",
        "\n",
        "for number in range(len(X_searchspace)):\n",
        "    m_ii, std_ii   = GP_m.GP_inference_np(X_searchspace[number])\n",
        "    Ysearchspace_mean[number] = m_ii.item()\n",
        "    Ysearchspace_std[number]  = std_ii.item()\n",
        "\n",
        "\n",
        "ax = plt.figure(figsize = (8,4), dpi = 100)\n",
        "plt.plot(X_training, Y_training  , 'kx', mew=2)\n",
        "plt.plot(X_searchspace,Ysearchspace_mean, 'C0', lw=2)\n",
        "plt.gca().fill_between(X_searchspace.flat,\n",
        "                       Ysearchspace_mean  - 3*np.sqrt(Ysearchspace_std),\n",
        "                       Ysearchspace_mean  + 3*np.sqrt(Ysearchspace_std),\n",
        "                       color='C0', alpha=0.2)\n",
        "\n",
        "#####\n",
        "#Obtain full mean and covariance matrix of interested search space, obtain posteriors and plot\n",
        "#Write your code here!\n",
        "#####\n",
        "\n",
        "plt.title('Gaussian Process Regression')\n",
        "plt.legend(('Training Data', 'Objective Function', 'GP mean', 'GP Confidence Interval'),\n",
        "           loc='lower right')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5bab2401-2168-438c-af48-dca6080c43bd",
      "metadata": {
        "id": "5bab2401-2168-438c-af48-dca6080c43bd"
      },
      "outputs": [],
      "source": [
        "# You can use your own GP class or use the class below\n",
        "# If using your own GP class - remember to modify the ...\n",
        "# 'GP_m = GP_model_meanzero(X_training, Y_training, 'SquaredExponential', hyperparams_multistart_loops = 3)'\n",
        "# ... section of the above code! (And do any other troubleshooting - hence would recommend using the code below for ease')\n",
        "\n",
        "class GP_model_meanzero:\n",
        "\n",
        "    def __init__(self, X, Y, kernel, hyperparams_multistart_loops):\n",
        "        self.X, self.Y, self.kernel = X, Y, kernel\n",
        "        self.number_of_point, self.nx_dimensions, self.ny_dimensions = X.shape[0], X.shape[1], Y.shape[1]\n",
        "        self.multistart_loops            = hyperparams_multistart_loops\n",
        "\n",
        "        #Normalisation\n",
        "        self.X_mean, self.X_std     = np.mean(X, axis=0), np.std(X, axis=0)\n",
        "        self.Y_mean, self.Y_std     = np.mean(Y, axis=0), np.std(Y, axis=0)\n",
        "        self.X_norm, self.Y_norm    = (X-self.X_mean)/self.X_std, (Y-self.Y_mean)/self.Y_std\n",
        "\n",
        "        #Determine Kernel Hyperparameters\n",
        "        self.hyperparam_optimized , self.inverse_covariance_matrix_opt   = self.determine_hyperparameters()\n",
        "\n",
        "    def Cov_mat(self, kernel, X_norm, W, sf2):\n",
        "        if kernel == 'SquaredExponential':\n",
        "            xixj_euclidean_distance = cdist(X_norm, X_norm, 'seuclidean', V=W)**2\n",
        "            cov_matrix = sf2*np.exp(-0.5*xixj_euclidean_distance)\n",
        "            return (cov_matrix)\n",
        "        else:\n",
        "            print('ERROR no kernel with name ', kernel)\n",
        "\n",
        "\n",
        "    def negative_loglikelihood(self, hyper, X, Y):\n",
        "        # internal parameters\n",
        "        n_point, nx_dim = self.number_of_point, self.nx_dimensions\n",
        "        kernel          = self.kernel\n",
        "\n",
        "        W               = np.exp(2*hyper[:nx_dim])   # W <=> 1/lambda\n",
        "        sf2             = np.exp(2*hyper[nx_dim])    # variance of the signal\n",
        "        sn2             = np.exp(2*hyper[nx_dim+1])  # variance of noise\n",
        "\n",
        "        # obtaining negative logliklihood via Cholesky decomposition\n",
        "        K       = self.Cov_mat(kernel, X, W, sf2)\n",
        "        K       = K + (sn2 + 1e-8)*np.eye(n_point)\n",
        "        K       = (K + K.T)*0.5\n",
        "        L       = np.linalg.cholesky(K)\n",
        "        logdetK = 2 * np.sum(np.log(np.diag(L)))\n",
        "        invLY   = np.linalg.solve(L,Y)\n",
        "        alpha   = np.linalg.solve(L.T,invLY)\n",
        "        NLL     = np.dot(Y.T,alpha) + logdetK\n",
        "        return (NLL)\n",
        "\n",
        "\n",
        "    def determine_hyperparameters(self):\n",
        "        # setting up bounds for the log likelyhood minimsation\n",
        "        lower_bound = np.array([-4.]*(self.nx_dimensions+1) + [-8.])  # lengthscales + signal variance, noise variance\n",
        "        upper_bound = np.array([4.]*(self.nx_dimensions+1) + [ -2.])\n",
        "        bounds      = np.hstack((lower_bound.reshape(self.nx_dimensions+2,1), upper_bound.reshape(self.nx_dimensions+2,1)))\n",
        "\n",
        "        #gives number of input set of random starting guesses for each hyperparameter\n",
        "        multi_startvec         = sobol_seq.i4_sobol_generate(self.nx_dimensions + 2, self.multistart_loops)\n",
        "\n",
        "        #variables for storing information during loop\n",
        "        temp_min_hyperparams   = [0.]*self.multistart_loops\n",
        "        temp_loglikelihood     = np.zeros((self.multistart_loops))\n",
        "        hyperparam_optimized   = np.zeros((self.nx_dimensions+2, self.ny_dimensions)) #for best solutions\n",
        "        inverse_covariance_matrix_opt = []\n",
        "\n",
        "        #minimisation of hyperparameters\n",
        "        for i in range(self.ny_dimensions):\n",
        "            for j in range(self.multistart_loops ):\n",
        "                #initilising hyperparam guess\n",
        "                hyperparams_initialisation   = lower_bound + (upper_bound-lower_bound)*multi_startvec[j,:] # mapping sobol unit cube to boudns\n",
        "\n",
        "                result = minimize(self.negative_loglikelihood,\n",
        "                               hyperparams_initialisation,\n",
        "                               args=(self.X_norm, self.Y_norm[:,i]),\n",
        "                               method='SLSQP',\n",
        "                               options={'disp':False,'maxiter':10000},\n",
        "                               bounds=bounds,\n",
        "                               tol=1e-12)\n",
        "\n",
        "                temp_min_hyperparams[j] = result.x\n",
        "                temp_loglikelihood[j]   = result.fun\n",
        "\n",
        "            # choosing best solution from temporary lists\n",
        "            minimumloglikelihood_index    = np.argmin(temp_loglikelihood)\n",
        "            hyperparam_optimized[:,i]     = temp_min_hyperparams[minimumloglikelihood_index  ]\n",
        "\n",
        "            # exponential to recover value from log space\n",
        "            lengthscale_opt         = np.exp(2.*hyperparam_optimized[:self.nx_dimensions,i])\n",
        "            signalvarience_opt      = np.exp(2.*hyperparam_optimized[self.nx_dimensions,i])\n",
        "            noise_opt               = np.exp(2.*hyperparam_optimized[self.nx_dimensions+1,i]) + 1e-8\n",
        "\n",
        "            #obtain convarience matrix from optimised kernel hyper parameters\n",
        "            covarience_matrix_opt              = self.Cov_mat(self.kernel, self.X_norm, lengthscale_opt,signalvarience_opt) + noise_opt*np.eye(self.number_of_point)\n",
        "            self.covarience_matrix_opt         = covarience_matrix_opt\n",
        "            inverse_covariance_matrix_opt     += [np.linalg.solve(covarience_matrix_opt, np.eye(self.number_of_point))]\n",
        "\n",
        "        return (hyperparam_optimized , inverse_covariance_matrix_opt)\n",
        "\n",
        "    def calc_cov_sample(self,xnorm,Xnorm,ell,sf2):\n",
        "        # internal parameters\n",
        "        nx_dim = self.nx_dimensions\n",
        "\n",
        "        #covariance of sample\n",
        "        dist = cdist(Xnorm, xnorm.reshape(1,nx_dim), 'seuclidean', V=ell)**2\n",
        "        cov_matrix = sf2 * np.exp(-.5*dist)\n",
        "        return (cov_matrix )\n",
        "\n",
        "\n",
        "    def GP_inference_np(self, x):\n",
        "        nx_dim                   = self.nx_dimensions\n",
        "        kernel, ny_dim           = self.kernel, self.ny_dimensions\n",
        "        hypopt, Cov_mat          = self.hyperparam_optimized, self.Cov_mat\n",
        "        stdX, stdY, meanX, meanY = self.X_std, self.Y_std, self.X_mean, self.Y_mean\n",
        "        calc_cov_sample          = self.calc_cov_sample\n",
        "        invKsample               = self.inverse_covariance_matrix_opt\n",
        "        Xsample, Ysample         = self.X_norm, self.Y_norm\n",
        "\n",
        "        xnorm = (x - meanX)/stdX\n",
        "        mean  = np.zeros(ny_dim)\n",
        "        var   = np.zeros(ny_dim)\n",
        "\n",
        "        # ny_dim -> number of outputs\n",
        "        for i in range(ny_dim):\n",
        "            invK           = invKsample[i]\n",
        "            hyper          = hypopt[:,i]\n",
        "            ellopt, sf2opt = np.exp(2*hyper[:nx_dim]), np.exp(2*hyper[nx_dim])\n",
        "\n",
        "            # calculation of covaraince for each output\n",
        "            # although noise hyperparameter determiend, it is ignored here for better visualisation\n",
        "            k       = calc_cov_sample(xnorm,Xsample,ellopt,sf2opt)\n",
        "            mean[i] = np.matmul(np.matmul(k.T,invK),Ysample[:,i])\n",
        "            var[i]  = max(0, sf2opt - np.matmul(np.matmul(k.T,invK),k))\n",
        "\n",
        "        # un-normalisation (obtianing actual mean and variance)\n",
        "        mean_sample = mean*stdY + meanY\n",
        "        var_sample  = var*stdY**2\n",
        "\n",
        "        return (mean_sample, var_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f650d6-b62d-4796-9d46-d82d364d7107",
      "metadata": {
        "id": "88f650d6-b62d-4796-9d46-d82d364d7107"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a361e268-d7ed-46ca-ad19-bb891b1e14ea",
      "metadata": {
        "id": "a361e268-d7ed-46ca-ad19-bb891b1e14ea"
      },
      "source": [
        "#### Excersise A1_4:\n",
        "##### Concepts: Thompson Sampling, Batch BO\n",
        "\n",
        "Now that we can draw samples of the posterior, we can now adapt the `BO_1D` class below to allow for batch BO!\n",
        "\n",
        "Similar to the LCB acquisition function, we can draw recommendations from taking the minima (to perform minimisation) from each of the sampled posteriors. Can you modify the `BO_1D` class with the following to allow for batch BO?\n",
        "\n",
        "1. Add a new 'batch' input to specify the number of recommendations per iteration\n",
        "2. Create a new 'ThompsonSamnpling' acquisition function which draws the number of posterior sample specified by the 'batch_number' and obtain the input that correspond with the posterior minimum. `np.argmin()` function can be useful. It returns the index of the minimum number form a list.\n",
        "3. Modify the `plt.plot(range(iterations), self.exploredY)` function to a scatter plot to observe the batch evaluations over the iterations.\n",
        "\n",
        "```python\n",
        "class BO_1D:\n",
        "    def __init__(self, X, kernel, X_searchspace,  iterations, acquisition_function, objective_func, print_graph, acquisition_hyperparam):       \n",
        "        \n",
        "        self.X = X\n",
        "\n",
        "        Fx_training              = np.array([objective_func(x) for x in self.X])\n",
        "        self.Y                   = Fx_training.reshape(Fx_training.shape[0],1)\n",
        "        \n",
        "        fx_searchspace           = np.array([objective_func(x) for x in X_searchspace])\n",
        "        Ysearchspace_mean        = np.zeros(number_points_searchspace**(np.shape(X_training)[1]))\n",
        "        Ysearchspace_std         = np.zeros(number_points_searchspace**(np.shape(X_training)[1]))\n",
        "        \n",
        "        self.minY = []\n",
        "        self.exploredY = []\n",
        "        \n",
        "        for i in range(iterations):\n",
        "            GP_m = GP.GP_model_meanzero(self.X, self.Y, kernel, hyperparams_multistart_loops = 3)\n",
        "            \n",
        "            for number in range(len(X_searchspace)):\n",
        "                m_ii, std_ii   = GP_m.GP_inference_np(X_searchspace[number])\n",
        "                Ysearchspace_mean[number] = m_ii.item()\n",
        "                Ysearchspace_std[number]  = std_ii.item()\n",
        "            \n",
        "            if acquisition_function == 'Thompson':\n",
        "               ... # Add thompson sampling code here!\n",
        "                \n",
        "            else:\n",
        "                print('No acquisition function called ', acquisition_function)\n",
        "                break\n",
        "            \n",
        "            self.X = np.append(self.X, [[X_acquisitionfunc]],0)\n",
        "            self.Y = np.append(self.Y, [[objective_func(X_acquisitionfunc)]],0)\n",
        "\n",
        "            self.minY += [min(self.Y)]\n",
        "            self.exploredY += [objective_func(X_acquisitionfunc)]\n",
        "\n",
        "            if print_graph == True:\n",
        "                pass\n",
        "            else:\n",
        "                pass\n",
        "        plt.figure(figsize = (8,4), dpi = 100)\n",
        "        plt.title('Minimum of Training Data set over Iterations')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.plot(range(iterations), self.minY)\n",
        "        plt.show()\n",
        "        plt.figure(figsize = (8,4), dpi = 100)\n",
        "        plt.title('Evaluation Output over Iterations')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.plot(range(iterations), self.exploredY)\n",
        "        plt.show()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da84d5d9-ce0d-4977-bd2b-74418f23a07e",
      "metadata": {
        "id": "da84d5d9-ce0d-4977-bd2b-74418f23a07e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4c20c3cd-698c-4e79-ab39-08a9d05dc072",
      "metadata": {
        "id": "4c20c3cd-698c-4e79-ab39-08a9d05dc072"
      },
      "source": [
        "#### Excersise A1_5:\n",
        "##### Concepts: Thompson Sampling, Batch BO\n",
        "\n",
        "A code for the same implimentation of batch BO with thompson sampling is given below. We will run BO in 2 and 3 dimensions with the inputs specified below. Observe how much time it will take for only 2 rounds of BO with only 20 points for each input dimensions! Why does it take so much longer when going to higher dimension?\n",
        "\n",
        "*warning: 3 dimension computation could take a few minutes\n",
        "\n",
        "``` python\n",
        "class BO_multiD_batch:\n",
        "    def __init__(self, X, kernel, X_searchspace, iterations,acquisition_function, objective_func,print_graph, acquisition_hyperparam, batch):\n",
        "\n",
        "        # 1) initilisation\n",
        "        self.X = np.array(X)\n",
        "        # 2) build Y as an (n×1) vector\n",
        "        Fx_training = np.array([objective_func(x) for x in self.X])\n",
        "        self.Y = Fx_training.reshape(-1, 1)\n",
        "\n",
        "        self.minY = []\n",
        "        self.exploredY = []\n",
        "\n",
        "        for i in range(iterations):\n",
        "            # 3) fit GP to the current (X,Y)\n",
        "            GP_m = GP_model_meanzero(self.X, self.Y, kernel, hyperparams_multistart_loops=3)\n",
        "\n",
        "            # 4) predict over the search space\n",
        "            means = np.zeros(len(X_searchspace))\n",
        "            vars  = np.zeros(len(X_searchspace))\n",
        "            for idx, xx in enumerate(X_searchspace):\n",
        "                m, v = GP_m.GP_inference_np(xx)\n",
        "                means[idx] = m.item()\n",
        "                vars[idx]  = v.item()\n",
        "\n",
        "            # 5) sample from the joint posterior for Thompson sampling\n",
        "            mu_plot, cov_plot = GP_predict_full(GP_m, X_searchspace)\n",
        "            samples = np.random.multivariate_normal(mu_plot, cov_plot, size=batch)\n",
        "\n",
        "            # 6) pick batch points\n",
        "            new_X = []\n",
        "            for s in samples:\n",
        "                choice = np.argmin(s)\n",
        "                new_X.append(X_searchspace[choice])\n",
        "            new_X = np.array(new_X)\n",
        "\n",
        "            # 7) evaluate objective and append to X,Y\n",
        "            new_Y = np.array([objective_func(x) for x in new_X]).reshape(-1, 1)\n",
        "            self.X = np.vstack([self.X, new_X])\n",
        "            self.Y = np.vstack([self.Y, new_Y])\n",
        "\n",
        "            self.minY     += [self.Y.min()]\n",
        "            self.exploredY += [new_Y.flatten().tolist()]\n",
        "\n",
        "            if print_graph:\n",
        "                print(f\"Iteration {i} complete.\")\n",
        "\n",
        "        # 8) finally, plot\n",
        "        plt.figure(figsize=(8,4), dpi=100)\n",
        "        plt.title('Minimum of Training Data set over Iterations')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.plot(range(iterations), self.minY)\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(8,4), dpi=100)\n",
        "        plt.title('Evaluation Output over Iterations')\n",
        "        plt.xlabel('Iterations')\n",
        "        for i, vals in enumerate(self.exploredY):\n",
        "            plt.scatter([i]*batch, vals)\n",
        "        plt.show()\n",
        "```\n",
        "\n",
        "2 dimension input:\n",
        "\n",
        "``` python\n",
        "x1loc  = [-4,  -1,  2]; x2loc = [-3, -2, 7]\n",
        "Xtrain = [[x,y] for x in x1loc for y in x2loc]\n",
        "X_training = np.array(Xtrain)\n",
        "\n",
        "number_points_pervariable      = 20\n",
        "number_points_searchspace   = number_points_pervariable ** (np.shape(X_training)[1])\n",
        "\n",
        "#Here, the interested search space is symmetrical, if it is non symmetrical, one can define each dimension individually and build the search space recursively\n",
        "X_searchspace     = np.linspace(-5, 8, num=number_points_pervariable)\n",
        "X_searchspace     = np.array([[x,y] for x in X_searchspace for y in X_searchspace])\n",
        "\n",
        "def obj_func(X):\n",
        "\treturn (np.sin(X[0])+np.sin(X[1]))\n",
        "\n",
        "BO_m = BO_multiD_batch(X = X_training,  \n",
        "   kernel = 'SquaredExponential',\n",
        "   X_searchspace = X_searchspace,\n",
        "   iterations = 2,\n",
        "   acquisition_function = 'Thompson',\n",
        "   objective_func = obj_func,\n",
        "   print_graph = True,\n",
        "   acquisition_hyperparam=[10],\n",
        "    batch = 5)\n",
        "\n",
        "```\n",
        "\n",
        "3 dimension input:\n",
        "\n",
        "``` python\n",
        "x1loc  = [-4,  -1]; x2loc = [-3, -2]; x3loc = [-3, 1]\n",
        "Xtrain = [[x,y,a] for x in x1loc for y in x2loc for a in x3loc]\n",
        "X_training = np.array(Xtrain)\n",
        "\n",
        "number_points_pervariable    = 20\n",
        "number_points_searchspace   = number_points_pervariable ** (np.shape(X_training)[1])\n",
        "\n",
        "#Here, the interested search space is symmetrical, if it is non symmetrical, one can define each dimension individually and build the search space recursively\n",
        "X_searchspace     = np.linspace(-5, 8, num=number_points_pervariable)\n",
        "X_searchspace     = np.array([[x,y,a] for x in X_searchspace for y in X_searchspace for a in X_searchspace])\n",
        "\n",
        "def obj_func(X):\n",
        "\treturn (np.sin(X[0])+np.sin(X[1])+np.cos(X[2]))\n",
        "\n",
        "BO_m = BO_multiD_batch(X = X_training,  \n",
        "   kernel = 'SquaredExponential',\n",
        "   X_searchspace = X_searchspace,\n",
        "   iterations = 2,\n",
        "   acquisition_function = 'Thompson',\n",
        "   objective_func = obj_func,\n",
        "   print_graph = True,\n",
        "   acquisition_hyperparam=[10],\n",
        "    batch = 5)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d9323fc-3664-484e-b3eb-11ba75ff4b2e",
      "metadata": {
        "id": "0d9323fc-3664-484e-b3eb-11ba75ff4b2e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "717fe70f-5380-4be7-a427-fa043ac3b9b2",
      "metadata": {
        "id": "717fe70f-5380-4be7-a427-fa043ac3b9b2"
      },
      "source": [
        "#### Excersise A1_6:\n",
        "##### Concepts: Marginal Thompson Sampling, Batch BO\n",
        "\n",
        "We can attibute the long run times to the need for the full covariance matrix to be evaluated in `GP_predict_full()`. Another method to obtain batach recommendations is through a tecnique called 'Marginal Thompson Sampling'. Here, we are not obtaining posterior samples. Instead, we can draw individual samples of each point with the mean and standard deviation of each point (no covariance required) with the standard deviation scaled by a hyperparameter (similar to LCB) and a scalar drawn from the standard normal distribution via `np.random.randn()`. We can observe and example of 5 batch samples and recommendations in the following plot.\n",
        "\n",
        "![download.png](attachment:6da18bad-6429-4a02-8f05-a206b96cfbd5.png)\n",
        "\n",
        "An implimentation to obtain Marginal TS samples is given below. Similar to excersise A1_4, can you modify your previous `BO_1D_batch` function to perform batch BO via marginal TS? (You do not have to use the given code if it does not fit with your previous implimentation.)\n",
        "\n",
        "``` python\n",
        "\n",
        "for number in range(batch):\n",
        "    if acquisition_function == 'ThompsonMarginal':\n",
        "        ts_sample  = Ysearchspace_mean + acquisition_hyperparam[0]*np.sqrt(Ysearchspace_std)*np.random.randn(X_searchspace.shape[0])\n",
        "        new_X.append(X_searchspace[np.argmin(ts_sample)])\n",
        "        plt.plot(X_searchspace,ts_sample, lw = 0.2, alpha = 0.3)\n",
        "    else:\n",
        "        print('No acquisition function called ', acquisition_function)\n",
        "        break\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Use the following code to test your batch BO code.\n",
        "\n",
        "``` python\n",
        "def obj_func(x):\n",
        "\treturn np.sin(x**np.sin(x))\n",
        "\n",
        "# training data\n",
        "X_training = np.array([3.5, 4])\n",
        "ndata      = X_training.shape[0]\n",
        "X_training = X_training.reshape(ndata,1)\n",
        "\n",
        "# search space\n",
        "number_points_searchspace  = 500\n",
        "X_searchspace              = np.linspace(3, 8.0, num=number_points_searchspace)\n",
        "\n",
        "BO_m = BO_1D_batch(X = X_training,  \n",
        "   kernel = 'SquaredExponential',\n",
        "   X_searchspace = X_searchspace,\n",
        "   iterations = 25,\n",
        "   acquisition_function = 'ThompsonMarginal',\n",
        "   objective_func = obj_func,\n",
        "   print_graph = True,\n",
        "   acquisition_hyperparam=[3],\n",
        "   batch = 5)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2c227cd-a4c6-4039-a2c3-abbe4073bac0",
      "metadata": {
        "id": "e2c227cd-a4c6-4039-a2c3-abbe4073bac0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7304966b-83a4-4710-b853-8fd2dea60e28",
      "metadata": {
        "id": "7304966b-83a4-4710-b853-8fd2dea60e28"
      },
      "source": [
        "#### Excersise A1_7:\n",
        "##### Concepts: Marginal Thompson Sampling, Batch BO\n",
        "\n",
        "Similar to excersise A1_5, some code for the multidimensional batch BO is given below. Use this code and explore how one could perform multidimensional BO (go up to 5/6 dimensions with your chosen objective function! See how the various parts of the inputs (hyperparameters etc.) and code perform with varying values! How long does the code take to run!). An example with a 2D input is shown.\n",
        "\n",
        "```python\n",
        "class BO_multiD_batch:\n",
        "    def __init__(self, X, kernel, X_searchspace, iterations,acquisition_function, objective_func,print_graph, acquisition_hyperparam, batch):\n",
        "\n",
        "        # 1) initilisation\n",
        "        self.X = np.array(X)\n",
        "        # 2) build Y as an (n×1) vector\n",
        "        Fx_training = np.array([objective_func(x) for x in self.X])\n",
        "        self.Y = Fx_training.reshape(-1, 1)\n",
        "\n",
        "        self.minY = []\n",
        "        self.exploredY = []\n",
        "\n",
        "        for i in range(iterations):\n",
        "            # 3) fit GP to the current (X,Y)\n",
        "            GP_m = GP_model_meanzero(self.X, self.Y, kernel, hyperparams_multistart_loops=3)\n",
        "\n",
        "            # 4) predict over the search space\n",
        "            means = np.zeros(len(X_searchspace))\n",
        "            varience  = np.zeros(len(X_searchspace))\n",
        "            for idx, xx in enumerate(X_searchspace):\n",
        "                m, v = GP_m.GP_inference_np(xx)\n",
        "                means[idx] = m.item()\n",
        "                varience[idx]  = v.item()\n",
        "\n",
        "            # 5) sample from the joint posterior for Thompson sampling\n",
        "            if acquisition_function == 'ThompsonMarginal':\n",
        "                new_X = []\n",
        "                for number in range(batch):\n",
        "                    ts_sample  = means + acquisition_hyperparam[0]*np.sqrt(varience)*np.random.randn(X_searchspace.shape[0])\n",
        "                    new_X.append(X_searchspace[np.argmin(ts_sample)])\n",
        "                new_X = np.array(new_X)\n",
        "            else:\n",
        "                print('No acquisition function named', acquisition_function)\n",
        "            \n",
        "            # 7) evaluate objective and append to X,Y\n",
        "            new_Y = np.array([objective_func(x) for x in new_X]).reshape(-1, 1)\n",
        "            self.X = np.vstack([self.X, new_X])\n",
        "            self.Y = np.vstack([self.Y, new_Y])\n",
        "\n",
        "            self.minY     += [self.Y.min()]\n",
        "            self.exploredY += [new_Y.flatten().tolist()]\n",
        "\n",
        "            if print_graph:\n",
        "                print(f\"Iteration {i} complete.\")\n",
        "\n",
        "        # 8) finally, plot\n",
        "        plt.figure(figsize=(8,4), dpi=100)\n",
        "        plt.title('Minimum of Training Data set over Iterations')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.plot(range(iterations), self.minY)\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(8,4), dpi=100)\n",
        "        plt.title('Evaluation Output over Iterations')\n",
        "        plt.xlabel('Iterations')\n",
        "        for i, vals in enumerate(self.exploredY):\n",
        "            plt.scatter([i]*batch, vals)\n",
        "        plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "For 2D input:\n",
        "\n",
        "``` python\n",
        "x1loc  = [-4,  -1,  2]; x2loc = [-3, -2, 7]\n",
        "Xtrain = [[x,y] for x in x1loc for y in x2loc]\n",
        "X_training = np.array(Xtrain)\n",
        "\n",
        "number_points_pervariable      = 20\n",
        "number_points_searchspace   = number_points_pervariable ** (np.shape(X_training)[1])\n",
        "\n",
        "#Here, the interested search space is symmetrical, if it is non symmetrical, one can define each dimension individually and build the search space recursively\n",
        "X_searchspace     = np.linspace(-5, 8, num=number_points_pervariable)\n",
        "X_searchspace     = np.array([[x,y] for x in X_searchspace for y in X_searchspace])\n",
        "\n",
        "def obj_func(X):\n",
        "\treturn (np.sin(X[0])+np.sin(X[1]))\n",
        "\n",
        "BO_m = BO_multiD_batch(X = X_training,  \n",
        "   kernel = 'SquaredExponential',\n",
        "   X_searchspace = X_searchspace,\n",
        "   iterations = 20,\n",
        "   acquisition_function = 'ThompsonMarginal',\n",
        "   objective_func = obj_func,\n",
        "   print_graph = True,\n",
        "   acquisition_hyperparam=[10],\n",
        "   batch = 5)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5b6eda-7d3a-45c1-8a03-b6f660c4ff96",
      "metadata": {
        "id": "aa5b6eda-7d3a-45c1-8a03-b6f660c4ff96"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6b2ca152-ecea-4c04-9a13-05b570d4c5b9",
      "metadata": {
        "id": "6b2ca152-ecea-4c04-9a13-05b570d4c5b9"
      },
      "source": [
        "## Section B: Introduction to Reaction Optimisation with BO\n",
        "\n",
        "We are now going to use our batch BO workflow to run for an optimisation campaign for a chemical reaction! The case study can be found in this reference.\n",
        "\n",
        "![image.png](attachment:71fb980f-79e0-4f55-96be-9c020a0d5589.png)\n",
        "\n",
        "Alexander Buitrago Santanilla et al. ,Nanomole-scale high-throughput chemistry for the synthesis of complex molecules.Science347,49-53(2015).DOI:<a href=\"https://www.science.org/doi/10.1126/science.1259203\">10.1126/science.1259203</a>\n",
        "\n",
        "The reaction condition space for a particular chemical transformation can be extremely vast due to the combinatorial nature of variables (especially if the reaction is not established). For example, our case study is a Buchwald-Hartwig amination (C-N cross coupling) which requires a palladium catalyst, ligand, base and solvent. Just considering 10 chemicals for each reagent will lead to a space of 10000 possible reaction conditions!\n",
        "\n",
        "Bayesian Optimisation provides a systematic, data-driven way of choosing which experiments to perform. We will only be considering 2 variables in the optimisation of this reaction: The pre-catalyst and base. Our goal is to obtain the reaction condition with the highest yield (which combination of base and pre-catlayst gives the highest yield?). Below is the list of pre-catalyst and bases we will be examining.\n",
        "\n",
        "A word about the pre-catalyst: Pre-catalysts are precursors to the active catalyst for a catalystic reaction. They are often have the catalytic center (the metal atom) and the ligand pre-bonded such that the pre-catalyst can be directly used or dispense for a reaction. There are 4 generations to the palladium pre-catalyst. This case study uses the 2nd and 3rd generation of precatalyst. The structures of the pre-catlaysts, ligands and bases are seen below (L = ligand).\n",
        "\n",
        "![image.png](attachment:03e13af8-5308-432e-8e13-693dd531c9f4.png)\n",
        "![image.png](attachment:75f349bd-c554-4793-bc70-d021d02ff3fd.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabc10f3-f8b4-4d9e-a1b8-24f8c4971e5c",
      "metadata": {
        "id": "eabc10f3-f8b4-4d9e-a1b8-24f8c4971e5c"
      },
      "outputs": [],
      "source": [
        "precatalyst = ['G3_BINAP',\n",
        "               'G3_DPPF',\n",
        "               'G2_XantPhos',\n",
        "               'G2_tertBu3P',\n",
        "               'G3_PPA',\n",
        "               'G3_Aphos',\n",
        "               'G3_Xphos',\n",
        "               'G2_RuPhos',\n",
        "               'G3_DTBPF',\n",
        "               'G3_J009',\n",
        "               'G3_MorDalPhos',\n",
        "               'G3_BrettPhos',\n",
        "               'G3_tertBuXPhos',\n",
        "               'G3_tertBuBrettPhos',\n",
        "               'G3_RockPhos',\n",
        "               'G3_AdBrettPhos']\n",
        "base = ['DBU',\n",
        "        'MTBD',\n",
        "        'BTMG',\n",
        "        'BEMP',\n",
        "        'BTTP',\n",
        "        'P2Et']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c26559f0-db4b-4852-8bf4-0c686231090f",
      "metadata": {
        "id": "c26559f0-db4b-4852-8bf4-0c686231090f"
      },
      "source": [
        "#### Excersise B1_1:\n",
        "##### Concepts: Chemical Descriptors\n",
        "\n",
        "Before perfoming BO, we have to convert the pre-catalysts and bases into a machine-readable form. GP regressors are not able to process each reagent just from the names. We have to translate the chemical identities into a numerical number such that the GP can not only interpret them but also draw relations between them (via the mean and covariance matrix). There are many ways one can generate descriptor for each molecule.\n",
        "\n",
        "A few include the use of density functional theory descriptors, cheminformatics descriptors, fingerprinting, experimental values etc. There are whole databases dedicated to the generation and storage of descriptors. <a href=\"https://descriptor-libraries.molssi.org/kraken/\">An example is the KRAKEN data base for ligands. </a>\n",
        "\n",
        "The best descriptors are ones which directly relate to the outcome of the reaction. For example, if a base of higher pKaH value leads to a higher yield, then pKaH would be a good descriptor. However, it is rare to know prior to the optimisation of the reaction (or similar reactions) if a descriptors is suitable. There are many examples of chemical optimisation of BO that take the dimensionally reduced components of a very long list of descriptors.\n",
        "\n",
        "However, for demonstration purposes, we will stick to some simple descriptors such as molecular mass for the pre-catalyst and pKaH for the bases.\n",
        "\n",
        "```python\n",
        "precatalyst_descriptors = [['G3_BINAP',[622.20]],\n",
        "                           ['G3_DPPF',[544.10]],\n",
        "                           ['G2_XantPhos',[578.19]],\n",
        "                           ['G2_tertBu3P',[202.19]],\n",
        "                           ['G3_PPA',[292.12]],\n",
        "                           ['G3_Aphos',[265.20]],\n",
        "                           ['G3_Xphos',[476.36]],\n",
        "                           ['G2_RuPhos',[466.30]],\n",
        "                           ['G3_DTBPF',[474.23]],\n",
        "                           ['G3_J009',[554.29]],\n",
        "                           ['G3_MorDalPhos',[463.30]],\n",
        "                           ['G3_BrettPhos',[536.38]],\n",
        "                           ['G3_tertBuXPhos',[424.33]],\n",
        "                           ['G3_tertBuBrettPhos',[484.35]],\n",
        "                           ['G3_RockPhos',[468.35]],\n",
        "                           ['G3_AdBrettPhos',[640.44]]]\n",
        "\n",
        "base_descriptors = [['DBU', [24.3]],\n",
        "                    ['MTBD', [25.5]],\n",
        "                    ['BTMG', [23.6]],\n",
        "                    ['BEMP', [27.6]],\n",
        "                    ['BTTP', [28.4]],\n",
        "                    ['P2Et', [32.9]]]\n",
        "```\n",
        "\n",
        "Can you find some alternative descriptors for the precatalyst and base? We will used this to perform BO and evaluate which set of descriptors are better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9dd3052-cc67-4639-8eee-a978bf481e8e",
      "metadata": {
        "id": "b9dd3052-cc67-4639-8eee-a978bf481e8e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "052256b3-2f33-4f06-9955-93d43ac520f7",
      "metadata": {
        "id": "052256b3-2f33-4f06-9955-93d43ac520f7"
      },
      "source": [
        "#### Excersise B1_2:\n",
        "##### Concepts: Optimisation of Chemical Reactions with BO\n",
        "\n",
        "We can now perform some BO! Usually, the evaluation of the reaction condition will involve experimentation. In this case, an objective function (with a delay in the BO class) will be used to emulate experimentation. This objective function takes in the descriptors and searches in the search space for the index with matching descriptors and returns the yield as a function of the index.\n",
        "\n",
        "It is therefore very important that the search space is built in the same order as the yield list! (the notebook will explicitly provide the order in which the search space is to be built.\n",
        "\n",
        "Since we have built a BO algorithm to minimise, the yields are inverted into the negative space such that minimisation will conrespond to obtaining the highest yield.\n",
        "\n",
        "```python\n",
        "reaction_yield = [-3.6, -3.9, -3.5, -3.3, -3.6, -3.0, -1.1, -0.7, -0.8, -0.6, -0.8, -1.2, -0.7, -1.5, -0.0, -0.0, -0.0, -0.5, -5.0, -0.0, -0.0, -1.6, -3.5, -6.6, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.5, -1.5, -0.0, -0.0, -0.0, -0.7, -0.0, -1.6, -0.0, -1.3, -2.2, -23.2, -0.0, -0.0, -0.0, -1.9, -4.0, -16.3, -4.0, -1.0, -0.0, -0.0, -0.0, -62.7, -0.4, -1.8, -1.7, -1.0, -0.0, -0.0, -0.1, -0.3, -0.3, -0.4, -0.4, -2.8, -1.1, -1.4, -2.6, -0.7, -1.4, -2.1, -10.9, -91.7, -100.0, -50.2, -27.8, -79.1, -0.0, -3.1, -0.0, -1.2, -1.8, -14.5, -0.5, -0.5, -0.0, -0.4, -0.6, -7.8, -3.1, -3.5, -3.2, -2.2, -3.0, -15.8]\n",
        "\n",
        "def experiment_yield(condition , X_searchspace):\n",
        "    return(reaction_yield[X_searchspace.index(condition)])\n",
        "\n",
        "#ordering of the search space is very important\n",
        "X_searchspace = []\n",
        "for precat in precatalyst_descriptors:\n",
        "    for base in base_descriptors:\n",
        "        X_searchspace += [precat[1]+base[1]]\n",
        "\n",
        "```\n",
        "\n",
        "Code for batch BO using marginal Thompson Sampling and Squared Exponential kernel functions is observed below.\n",
        "\n",
        "```python\n",
        "class BO_multiD_batch_chemistry:\n",
        "    def __init__(self, X, kernel, X_searchspace, iterations,acquisition_function, objective_func,print_graph, acquisition_hyperparam, batch, experiment_time):\n",
        "\n",
        "        self.X = np.array(X)\n",
        "        Fx_training = np.array([objective_func(x.tolist(),X_searchspace) for x in self.X])\n",
        "        self.Y = Fx_training.reshape(-1, 1)\n",
        "        \n",
        "        self.minY = []\n",
        "        self.exploredY = []\n",
        "\n",
        "        for i in range(iterations):\n",
        "            GP_m = GP_model_meanzero(self.X, self.Y, kernel, hyperparams_multistart_loops=3)\n",
        "\n",
        "            means = np.zeros(len(X_searchspace))\n",
        "            varience  = np.zeros(len(X_searchspace))\n",
        "            for idx, xx in enumerate(X_searchspace):\n",
        "                m, v = GP_m.GP_inference_np(xx)\n",
        "                means[idx] = m.item()\n",
        "                varience[idx]  = v.item()\n",
        "\n",
        "            \n",
        "            if acquisition_function == 'ThompsonMarginal':\n",
        "                new_X = []\n",
        "                for number in range(batch):\n",
        "                    ts_sample  = means + acquisition_hyperparam[0]*np.sqrt(varience)*np.random.randn(len(X_searchspace))\n",
        "                    new_X.append(X_searchspace[np.argmin(ts_sample)])\n",
        "                new_X = np.array(new_X)\n",
        "            else:\n",
        "                print('No acquisition function named', acquisition_function)\n",
        "\n",
        "            time.sleep(experiment_time)\n",
        "            new_Y = np.array([objective_func(x.tolist(),X_searchspace) for x in new_X]).reshape(-1, 1)\n",
        "            self.X = np.vstack([self.X, new_X])\n",
        "            self.Y = np.vstack([self.Y, new_Y])\n",
        "\n",
        "            self.minY     += [self.Y.min()]\n",
        "            self.exploredY += [new_Y.flatten().tolist()]\n",
        "\n",
        "            if print_graph:\n",
        "                print(f\"Iteration {i} complete.\")\n",
        "\n",
        "        plt.figure(figsize=(8,4), dpi=100)\n",
        "        plt.title('Minimum of Training Data set over Iterations')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.plot(range(iterations), self.minY)\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(8,4), dpi=100)\n",
        "        plt.title('Evaluation Output over Iterations')\n",
        "        plt.xlabel('Iterations')\n",
        "        for i, vals in enumerate(self.exploredY):\n",
        "            plt.scatter([i]*batch, vals)\n",
        "        plt.show()\n",
        "\n",
        "BO_m = BO_multiD_batch_chemistry(X = X_training,  \n",
        "                                 kernel = 'SquaredExponential',\n",
        "                                 X_searchspace = X_searchspace,\n",
        "                                 iterations = 15,\n",
        "                                 acquisition_function = 'ThompsonMarginal',\n",
        "                                 objective_func = experiment_yield,\n",
        "                                 print_graph = True,\n",
        "                                 acquisition_hyperparam=[3],\n",
        "                                 batch = 5,\n",
        "                                 experiment_time = 0)\n",
        "\n",
        "```\n",
        "\n",
        "Using this code, start your first reaction optimisation campaign using BO! (We can leave the experiment_time to 0 for now.) Start with 3 training data points - these must be in the correct order of precatayst descriptors first then base descriptors. For example:\n",
        "\n",
        "```python\n",
        "X_training = [[544.1, 32.9],[265.2, 24.3],[640.44, 25.5]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5393002f-2f15-46ae-9cea-0d5fe4fdcde8",
      "metadata": {
        "id": "5393002f-2f15-46ae-9cea-0d5fe4fdcde8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fb0aa14a-f6e3-43da-9047-3180ff1e4167",
      "metadata": {
        "id": "fb0aa14a-f6e3-43da-9047-3180ff1e4167"
      },
      "source": [
        "#### Excersise B1_3:\n",
        "##### Concepts: Optimisation of Chemical Reactions with BO\n",
        "\n",
        "We can also use multidimensional desciptors for each molecule! This is usually beneficial especially when there are multiple factors that contribute towards the goal. (For example, when sterics and electronic properties of the molecule affects the outcome). However, as seen previously, the more dimensions, the longer the time the BO will take. For now, lets observe an example of a 2D input for the precatalyst.\n",
        "\n",
        "With higher dimensions, it is required that the training data include at least 2 different data points for the same dimension. Otherwise you will get an error (the covariance matrix cannot be formed).\n",
        "\n",
        "```python\n",
        "precatalyst_descriptors = [['G3_BINAP',[622.20, 2]],\n",
        "                           ['G3_DPPF',[544.10, 2]],\n",
        "                           ['G2_XantPhos',[578.19, 2]],\n",
        "                           ['G2_tertBu3P',[202.19, 1]],\n",
        "                           ['G3_PPA',[292.12, 1]],\n",
        "                           ['G3_Aphos',[265.20, 1]],\n",
        "                           ['G3_Xphos',[476.36, 1]],\n",
        "                           ['G2_RuPhos',[466.30, 1]],\n",
        "                           ['G3_DTBPF',[474.23, 2]],\n",
        "                           ['G3_J009',[554.29, 2]],\n",
        "                           ['G3_MorDalPhos',[463.30, 1]],\n",
        "                           ['G3_BrettPhos',[536.38, 1]],\n",
        "                           ['G3_tertBuXPhos',[424.33, 1]],\n",
        "                           ['G3_tertBuBrettPhos',[484.35, 1]],\n",
        "                           ['G3_RockPhos',[468.35, 1]],\n",
        "                           ['G3_AdBrettPhos',[640.44, 1]]]\n",
        "\n",
        "base_descriptors = [['DBU', [24.3]],\n",
        "                    ['MTBD', [25.5]],\n",
        "                    ['BTMG', [23.6]],\n",
        "                    ['BEMP', [27.6]],\n",
        "                    ['BTTP', [28.4]],\n",
        "                    ['P2Et', [32.9]]]\n",
        "\n",
        "X_training = [[544.1, 2, 32.9],[265.2, 1, 24.3],[640.44, 1, 25.5], [622.20, 2, 28.4]]\n",
        "X_searchspace = []\n",
        "for precat in precatalyst_descriptors:\n",
        "    for base in base_descriptors:\n",
        "        X_searchspace += [precat[1]+base[1]]\n",
        "\n",
        "BO_m = BO_multiD_batch_chemistry(X = X_training,  \n",
        "   kernel = 'SquaredExponential',\n",
        "   X_searchspace = X_searchspace,\n",
        "   iterations = 15,\n",
        "   acquisition_function = 'ThompsonMarginal',\n",
        "   objective_func = experiment_yield,\n",
        "   print_graph = True,\n",
        "   acquisition_hyperparam=[3],\n",
        "   batch = 5,\n",
        "   experiment_time=0)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1019cd-b1ad-47f5-a3a4-cc04e77161e1",
      "metadata": {
        "id": "7d1019cd-b1ad-47f5-a3a4-cc04e77161e1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cb8fc32c-e812-4b43-a38d-957ab3feab2d",
      "metadata": {
        "id": "cb8fc32c-e812-4b43-a38d-957ab3feab2d"
      },
      "source": [
        "#### Excersise B1_4:\n",
        "##### Concepts: Optimisation of Chemical Reactions with BO\n",
        "\n",
        "Well done! Now that you have succesfully run a BO optimisation campaign and known the various aspects of BO. Please explore how changing various aspects of the input (hyperparameters, descriptors, iterations vs batch size for a given budget etc.), changing the code (a different acquisition function etc.) affect the performance and runtime of the code.\n",
        "\n",
        "This will greatly help you in the coming hackathon!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f77360-99e7-4446-ac8d-46f5fc59153d",
      "metadata": {
        "id": "85f77360-99e7-4446-ac8d-46f5fc59153d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}