{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "60ca6f1d-d55c-41cc-b6f6-0acd8d3ab56b",
      "metadata": {
        "id": "60ca6f1d-d55c-41cc-b6f6-0acd8d3ab56b"
      },
      "source": [
        "## Section B: Gaussian Process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13171efb-6f65-4146-87d5-4c87446cf93c",
      "metadata": {
        "id": "13171efb-6f65-4146-87d5-4c87446cf93c"
      },
      "source": [
        "Gaussian Process is one of the most popular surrogate models for the Bayesian Optimisation workflow. The last question of Section A asked of what the similarities and differences are between the conditioning excersise and GP regression as seen in the figures below.\n",
        "\n",
        "Conditioning Excersise:\n",
        "![download.png](attachment:e10d6400-1c66-4ea7-9e92-deffab029f22.png)\n",
        "\n",
        "Gaussian Process Regession:\n",
        "![download.png](attachment:c6eafd65-effb-406d-b41e-33207014f8ba.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7dea70-6048-4e28-8da4-787e0728ec46",
      "metadata": {
        "id": "2e7dea70-6048-4e28-8da4-787e0728ec46"
      },
      "source": [
        "A key similarity is that the conditioning excersise used exact points to condition and to determine the conditioned mean and variance of other variables. This can be seen as data points which we use as training data for the Gaussian Process.\n",
        "\n",
        "A key difference is that in the Gaussian Process regression, we (usually) only have some data points and do not have the nessasry information (mean vector and covariance matrix) to relate and predict the conditioned mean and variance of other points. Hence, for a GP regression to work, we have to make some assumptions about the mean vector and covariance matrix to allow for conditioning.\n",
        "\n",
        "Also, another (perhaps more obvious) difference is that the conditioning excersise was not an excersise of regression. In this section, we will explore how GPs can be used as models to relate inputs to an output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f280642-18b7-446f-bc32-110213d5fc2a",
      "metadata": {
        "id": "9f280642-18b7-446f-bc32-110213d5fc2a"
      },
      "source": [
        "### Importing Relevant Packages\n",
        "(Feel free to import any packages you feel like needing to fully explore the content!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "062e0bba-1fba-4241-8435-6de9eb69a636",
      "metadata": {
        "id": "062e0bba-1fba-4241-8435-6de9eb69a636",
        "outputId": "761592a3-14fb-4049-9faa-6e38b66eb8c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sobol_seq\n",
            "  Downloading sobol_seq-0.2.0-py3-none-any.whl.metadata (273 bytes)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sobol_seq) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sobol_seq) (2.0.2)\n",
            "Downloading sobol_seq-0.2.0-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: sobol_seq\n",
            "Successfully installed sobol_seq-0.2.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.15.3)\n",
            "Collecting linear-operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from linear-operator>=0.6->gpytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->linear-operator>=0.6->gpytorch) (3.0.2)\n",
            "Downloading gpytorch-1.14-py3-none-any.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, nvidia-cusolver-cu12, linear-operator, gpytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gpytorch-1.14 jaxtyping-0.3.2 linear-operator-0.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 wadler-lindig-0.1.7\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n",
            "Downloading rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl (34.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.3.3\n"
          ]
        }
      ],
      "source": [
        "# if using google collab, run the following pip installs!\n",
        "!pip install sobol_seq\n",
        "!pip install plotly\n",
        "!pip install gpytorch\n",
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5db2fb13-8af1-489b-a1a2-967bf5ed4253",
      "metadata": {
        "id": "5db2fb13-8af1-489b-a1a2-967bf5ed4253",
        "outputId": "a40a5b8b-601a-400e-f14f-ef0d5102d35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sobol_seq'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1886697834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msobol_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sobol_seq'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "from scipy.integrate import quad\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.optimize import minimize\n",
        "import math\n",
        "import time\n",
        "import sobol_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33720659-15f1-42b7-8f36-192607b826c3",
      "metadata": {
        "id": "33720659-15f1-42b7-8f36-192607b826c3"
      },
      "source": [
        "### Gaussian Process Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993dc1dd-5105-4c55-bebd-cf006d4667ae",
      "metadata": {
        "id": "993dc1dd-5105-4c55-bebd-cf006d4667ae"
      },
      "source": [
        "Below is a full Gaussian Processes Regressor coded as a class - it might seem a little complicated, but do not worry! We will break the code down to gather an understanding of how the Gaussian Process is built. Feel free to copy te code and modify it to explore how changes to the code influences the Gaussian Process.\n",
        "\n",
        "There are many implimentations of a Gaussian Process Regressor. The following is one of many different implimentations.\n",
        "\n",
        "``` python\n",
        "class GP_model_meanzero:\n",
        "\n",
        "    def __init__(self, X, Y, kernel, hyperparams_multistart_loops):\n",
        "        self.X, self.Y, self.kernel = X, Y, kernel\n",
        "        self.number_of_point, self.nx_dimensions, self.ny_dimensions = X.shape[0], X.shape[1], Y.shape[1]\n",
        "        self.multistart_loops            = hyperparams_multistart_loops\n",
        "\n",
        "        #Normalisation\n",
        "        self.X_mean, self.X_std     = np.mean(X, axis=0), np.std(X, axis=0)\n",
        "        self.Y_mean, self.Y_std     = np.mean(Y, axis=0), np.std(Y, axis=0)\n",
        "        self.X_norm, self.Y_norm    = (X-self.X_mean)/self.X_std, (Y-self.Y_mean)/self.Y_std\n",
        "\n",
        "        #Determine Kernel Hyperparameters\n",
        "        self.hyperparam_optimized , self.inverse_covariance_matrix_opt   = self.determine_hyperparameters()     \n",
        "        \n",
        "    def Cov_mat(self, kernel, X_norm, W, sf2):\n",
        "        if kernel == 'SquaredExponential':\n",
        "            xixj_euclidean_distance = cdist(X_norm, X_norm, 'seuclidean', V=W)**2\n",
        "            cov_matrix = sf2*np.exp(-0.5*xixj_euclidean_distance)\n",
        "            return (cov_matrix)\n",
        "        else:\n",
        "            print('ERROR no kernel with name ', kernel)\n",
        "\n",
        "\n",
        "    def negative_loglikelihood(self, hyper, X, Y):\n",
        "        # internal parameters\n",
        "        n_point, nx_dim = self.number_of_point, self.nx_dimensions\n",
        "        kernel          = self.kernel\n",
        "         \n",
        "        W               = np.exp(2*hyper[:nx_dim])   # W <=> 1/lambda\n",
        "        sf2             = np.exp(2*hyper[nx_dim])    # variance of the signal\n",
        "        sn2             = np.exp(2*hyper[nx_dim+1])  # variance of noise\n",
        "\n",
        "        # obtaining negative logliklihood via Cholesky decomposition\n",
        "        K       = self.Cov_mat(kernel, X, W, sf2)  \n",
        "        K       = K + (sn2 + 1e-8)*np.eye(n_point)\n",
        "        K       = (K + K.T)*0.5                   \n",
        "        L       = np.linalg.cholesky(K)            \n",
        "        logdetK = 2 * np.sum(np.log(np.diag(L)))   \n",
        "        invLY   = np.linalg.solve(L,Y)             \n",
        "        alpha   = np.linalg.solve(L.T,invLY)       \n",
        "        NLL     = np.dot(Y.T,alpha) + logdetK     \n",
        "        return (NLL)\n",
        "\n",
        "    \n",
        "    def determine_hyperparameters(self):\n",
        "        # setting up bounds for the log likelyhood minimsation\n",
        "        lower_bound = np.array([-4.]*(self.nx_dimensions+1) + [-8.])  # lengthscales + signal variance, noise variance\n",
        "        upper_bound = np.array([4.]*(self.nx_dimensions+1) + [ -2.])\n",
        "        bounds      = np.hstack((lower_bound.reshape(self.nx_dimensions+2,1), upper_bound.reshape(self.nx_dimensions+2,1)))\n",
        "\n",
        "        #gives number of input set of random starting guesses for each hyperparameter\n",
        "        multi_startvec         = sobol_seq.i4_sobol_generate(self.nx_dimensions + 2, self.multistart_loops)\n",
        "        \n",
        "        #variables for storing information during loop\n",
        "        temp_min_hyperparams   = [0.]*self.multistart_loops\n",
        "        temp_loglikelihood     = np.zeros((self.multistart_loops))\n",
        "        hyperparam_optimized   = np.zeros((self.nx_dimensions+2, self.ny_dimensions)) #for best solutions\n",
        "        inverse_covariance_matrix_opt = []\n",
        "        \n",
        "        #minimisation of hyperparameters\n",
        "        for i in range(self.ny_dimensions):\n",
        "            for j in range(self.multistart_loops ):\n",
        "                #initilising hyperparam guess\n",
        "                hyperparams_initialisation   = lower_bound + (upper_bound-lower_bound)*multi_startvec[j,:] # mapping sobol unit cube to boudns\n",
        "               \n",
        "                result = minimize(self.negative_loglikelihood,\n",
        "                               hyperparams_initialisation,\n",
        "                               args=(self.X_norm, self.Y_norm[:,i]),\n",
        "                               method='SLSQP',\n",
        "                               options={'disp':False,'maxiter':10000},\n",
        "                               bounds=bounds,\n",
        "                               tol=1e-12)\n",
        "                \n",
        "                temp_min_hyperparams[j] = result.x\n",
        "                temp_loglikelihood[j]   = result.fun  \n",
        "\n",
        "            # choosing best solution from temporary lists\n",
        "            minimumloglikelihood_index    = np.argmin(temp_loglikelihood)\n",
        "            hyperparam_optimized[:,i]     = temp_min_hyperparams[minimumloglikelihood_index  ]\n",
        "    \n",
        "            # exponential to recover value from log space\n",
        "            lengthscale_opt         = np.exp(2.*hyperparam_optimized[:self.nx_dimensions,i])\n",
        "            signalvarience_opt      = np.exp(2.*hyperparam_optimized[self.nx_dimensions,i])\n",
        "            noise_opt               = np.exp(2.*hyperparam_optimized[self.nx_dimensions+1,i]) + 1e-8\n",
        "    \n",
        "            #obtain convarience matrix from optimised kernel hyper parameters\n",
        "            covarience_matrix_opt              = self.Cov_mat(self.kernel, self.X_norm, lengthscale_opt,signalvarience_opt) + noise_opt*np.eye(self.number_of_point)\n",
        "            self.covarience_matrix_opt         = covarience_matrix_opt\n",
        "            inverse_covariance_matrix_opt     += [np.linalg.solve(covarience_matrix_opt, np.eye(self.number_of_point))]\n",
        "            \n",
        "        return (hyperparam_optimized , inverse_covariance_matrix_opt)\n",
        "\n",
        "    def calc_cov_sample(self,xnorm,Xnorm,ell,sf2):\n",
        "        # internal parameters\n",
        "        nx_dim = self.nx_dimensions\n",
        "\n",
        "        #covariance of sample\n",
        "        dist = cdist(Xnorm, xnorm.reshape(1,nx_dim), 'seuclidean', V=ell)**2\n",
        "        cov_matrix = sf2 * np.exp(-.5*dist)\n",
        "        return (cov_matrix )         \n",
        "\n",
        "\n",
        "    def GP_inference_np(self, x):\n",
        "        nx_dim                   = self.nx_dimensions\n",
        "        kernel, ny_dim           = self.kernel, self.ny_dimensions\n",
        "        hypopt, Cov_mat          = self.hyperparam_optimized, self.Cov_mat\n",
        "        stdX, stdY, meanX, meanY = self.X_std, self.Y_std, self.X_mean, self.Y_mean\n",
        "        calc_cov_sample          = self.calc_cov_sample\n",
        "        invKsample               = self.inverse_covariance_matrix_opt\n",
        "        Xsample, Ysample         = self.X_norm, self.Y_norm\n",
        "\n",
        "        xnorm = (x - meanX)/stdX\n",
        "        mean  = np.zeros(ny_dim)\n",
        "        var   = np.zeros(ny_dim)\n",
        "        \n",
        "        # ny_dim -> number of outputs\n",
        "        for i in range(ny_dim):\n",
        "            invK           = invKsample[i]\n",
        "            hyper          = hypopt[:,i]\n",
        "            ellopt, sf2opt = np.exp(2*hyper[:nx_dim]), np.exp(2*hyper[nx_dim])\n",
        "\n",
        "            # calculation of covaraince for each output\n",
        "            # although noise hyperparameter determiend, it is ignored here for better visualisation\n",
        "            k       = calc_cov_sample(xnorm,Xsample,ellopt,sf2opt)\n",
        "            mean[i] = np.matmul(np.matmul(k.T,invK),Ysample[:,i])\n",
        "            var[i]  = max(0, sf2opt - np.matmul(np.matmul(k.T,invK),k))\n",
        "\n",
        "        # un-normalisation (obtianing actual mean and variance)\n",
        "        mean_sample = mean*stdY + meanY\n",
        "        var_sample  = var*stdY**2\n",
        "        \n",
        "        return (mean_sample, var_sample)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c537dd79-3c50-4917-9da4-4a18c7103b77",
      "metadata": {
        "id": "c537dd79-3c50-4917-9da4-4a18c7103b77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7f7ff4b4-adb1-44fa-a89b-bc533a1ffbad",
      "metadata": {
        "id": "7f7ff4b4-adb1-44fa-a89b-bc533a1ffbad"
      },
      "source": [
        "### B1: Covariance Matrix and Kernel Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa47c5ef-4547-4bbc-9da6-645063de7d5c",
      "metadata": {
        "id": "fa47c5ef-4547-4bbc-9da6-645063de7d5c"
      },
      "source": [
        "We are first exploring the covariance matrix. We have observed previously that the covariance matrix has diagonal elements which describes the variance of each input point and off diagonal points which describes the co-variance. In defining a covariance matrix that can describe generally the relationship between points, a suitable assumption is such that points in which are closer in the input space wil have a closer output. This means that points which are close are more correlated than points that are further away.\n",
        "\n",
        "Mathematically, we can describe a function $k({x^i}, {x^j})$ which takes in two points ${x^i}$ and ${x^j}$ in the input space and outputs the correlation. These functions are called kernel functions.  \n",
        "\n",
        "The covarience matrix can be then constructed as a N x N matrix of these kernel functions where N is the number of points in the input space.\n",
        "$$\n",
        "K(X, X) \\;=\\;\n",
        "\\begin{bmatrix}\n",
        "k\\bigl(x^{(1)},x^{(1)}\\bigr) & \\cdots & k\\bigl(x^{(1)},x^{(n_d)}\\bigr) \\\\[6pt]\n",
        "\\vdots                       & \\ddots & \\vdots                       \\\\[6pt]\n",
        "k\\bigl(x^{(n_d)},x^{(1)}\\bigr) & \\cdots & k\\bigl(x^{(n_d)},x^{(n_d)}\\bigr)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4353345-3cb3-41b8-955f-1897450d31b8",
      "metadata": {
        "id": "d4353345-3cb3-41b8-955f-1897450d31b8"
      },
      "source": [
        "#### Excersise B1_1:\n",
        "##### Concepts: Kernel Functions\n",
        "\n",
        "An example of a kernel function is the squared exponential function.\n",
        "\n",
        "For one dimension, the squared exponential has the function.\n",
        "\n",
        "$$\n",
        "k\\bigl(x^{(i)}, x^{(j)}\\bigr)\n",
        "\\;=\\;\n",
        "\\sigma_f^2\n",
        "\\exp\\!\\Bigl[-\\,w\\,\\bigl(x^{(i)} - x^{(j)}\\bigr)^2\\Bigr]\n",
        "$$\n",
        "\n",
        "There are two hyper parameters associated with the function.\n",
        "$\\sigma_f^2$ is the signal varience. $w$ is associated to the length scale $l$ by the relationship $l = 1/w$ . These parameters will be 'fitted' to the training data during a regular GP regressor routine. We will explore how the hyperparameters influence the kernel function outputs.\n",
        "\n",
        "For now, write a function for the 1 dimensional kernel function which takes in xi, xj, w and sv2 as the inputs and return the correlation\n",
        "\n",
        "```python\n",
        "def squared_exponential_1D(xi, xj, w, sv2):\n",
        "    return(...)\n",
        "```\n",
        "\n",
        "Using the function, plot a 3d plot of the correlation with xi and xj be derived from x_array where:\n",
        "``` python\n",
        "x_array = np.linspace (-10, 10, 500)\n",
        "\n",
        "#one can use a nested loop to differentiate i and j values. Example:\n",
        "for i in x_array:\n",
        "    for j in x_array:\n",
        "        ...\n",
        "```\n",
        "Let the $\\sigma_f^2 = 0.04$ and $w = 1$. It will be useful to visualize the relationship as a contour plot.\n",
        "\n",
        "What do you notice about the relationship between the distance of two input points and the correlation. Does it follow that the greater the difference, the lower the correlation?\n",
        "\n",
        "How does the rate of change of the correlation when going from points of low distance to points of high distannce? is it linear? quadratic? logarithmic? Hint: observe the equation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc51ce47-c69e-40f6-ac50-17b182382672",
      "metadata": {
        "id": "cc51ce47-c69e-40f6-ac50-17b182382672"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7e3080bb-d0f1-4726-a344-cab6f7e79055",
      "metadata": {
        "id": "7e3080bb-d0f1-4726-a344-cab6f7e79055"
      },
      "source": [
        "#### Excersise B1_2:\n",
        "##### Concepts: Kernel Hyperparameters\n",
        "We are now going to observe how the hyperperameters modify the output of the kernel function. Using the function you previously defined, plot the kernel function as a input of xi and xj with varying w and sf2.\n",
        "\n",
        "When varying w, what do you notice about the correlation between two input points? Does the correlation at 2 points which are far away increase or decrease with increasing w?\n",
        "\n",
        "When varying sf2, observe the correlation on the z axis. What is the maximum value of the correlation? How does it change when changing sf2?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11824f97-8714-4d61-8c64-5228ca9cbe53",
      "metadata": {
        "id": "11824f97-8714-4d61-8c64-5228ca9cbe53"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3d190095-5b4d-4b23-9daa-050eb5f6f864",
      "metadata": {
        "id": "3d190095-5b4d-4b23-9daa-050eb5f6f864"
      },
      "source": [
        "#### Excersise B1_3:\n",
        "##### Concepts: Multidimensional Kernel Function\n",
        "\n",
        "We now look the multidimensional form of the kernel function.\n",
        "\n",
        "$$\n",
        "k\\bigl(x^{(i)}, x^{(j)}\\bigr)\n",
        "\\;=\\;\n",
        "\\sigma_f^2\n",
        "\\exp\\!\\Bigl(-\\tfrac{1}{2}\\,(x^{(i)} - x^{(j)})^T \\,W\\, (x^{(i)} - x^{(j)})\\Bigr)\n",
        "$$\n",
        "\n",
        "We do not need to code a function for this as it is already written! Please observe the GP_model_meanzero class above to see where the multidimentsional kernel is implimented and how it is used to form the covariance matrix. Notice also how the training data is prepared via normalisation prior to the construction of the kernel.\n",
        "\n",
        "We can now use the class to obtain the covariance matrix of a training data set! We will use a 1D example with input $x$ and the objective function as $sin(x)$. We first have to prepare the training data. (the code in implimented in a way that takes inputs with shape of N x M where N is the number of data points and M is the number of dimensions. Then, we can build the GP_model_object and access attributes\n",
        "\n",
        "Please run: (please also execute the cell containing GP_model_meanzero class)\n",
        "``` python\n",
        "# define and reshape training data\n",
        "Xtrain = np.array([-4,-1.5,-1,0])\n",
        "ndata  = Xtrain.shape[0]\n",
        "Xtrain = Xtrain.reshape(ndata,1)\n",
        "\n",
        "Ytrain = np.sin(Xtrain)\n",
        "\n",
        "# build the GP object (instance of the DP_model_meanzero class)\n",
        "GP_m = GP_model_meanzero(Xtrain, Ytrain, 'SquaredExponential',  hyperparams_multistart_loops=3)\n",
        "```\n",
        "\n",
        "We will come to see how the full GP regressor is implimented. But use this oppertunity to be familar with how the input is formatted and the various attributes that one can call upon from the class.\n",
        "\n",
        "To access all the possible instance attributes of an object, please use the `vars()` fucntion on the object. To access an individual instance attribute, use the object method. For example, to observe the covariance matrix of the training data, we would execute:\n",
        "\n",
        "```python\n",
        "GP_m.covarience_matrix_opt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8700d40-f05d-462d-80bc-00fecb4c132c",
      "metadata": {
        "id": "f8700d40-f05d-462d-80bc-00fecb4c132c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3a233a04-dad6-4c66-92d6-b6449a51a7c2",
      "metadata": {
        "id": "3a233a04-dad6-4c66-92d6-b6449a51a7c2"
      },
      "source": [
        "#### Excersise B1_4:\n",
        "##### Concepts: Conditioning on Training Data\n",
        "\n",
        "Recall in Section A where it was shown how the mean and variance of other points can be obtained through conditioning.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(\\mathbf{x}_1\\mid\\mathbf{x}_2) &= \\mathcal{N}\\bigl(\\mathbf{x}_1;\\,\\boldsymbol{\\mu}_{x_1\\mid x_2},\\,\\Sigma_{x_1\\mid x_2}\\bigr)\\\\[6pt]\n",
        "\\boldsymbol{\\mu}_{x_1\\mid x_2} &= \\boldsymbol{\\mu}_{x_1} \\;+\\;\\Sigma_{x_1 x_2}\\,\\Sigma_{x_2}^{-1}\\,(\\mathbf{x}_2-\\boldsymbol{\\mu}_{x_2})\\\\[6pt]\n",
        "\\Sigma_{x_1\\mid x_2} &= \\Sigma_{x_1} \\;-\\;\\Sigma_{x_1 x_2}\\,\\Sigma_{x_2}^{-1}\\,\\Sigma_{x_2 x_1}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "With the covariance matrix obtained from the evaluation of the kernel function (we will see how the hyper parameters are learnt in the next section), we can now obtain the mean and variance of every input data points through a similar way - through conditioning!\n",
        "\n",
        "Mathematically, the mean and variance for point (${x_n}$) is obtained through a similar form by conditioning on input points $X$, output $Y$ and the covariance matrix $K(X,X)$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p\\bigl(f \\mid X, Y\\bigr)\n",
        "&= \\mathcal{GP}\\bigl(f;\\,\\mu_{XY}(\\cdot),\\,\\sigma^2_{XY}(\\cdot,\\cdot)\\bigr) \\\\[8pt]\n",
        "\\mu_{XY}(x_n)\n",
        "&= m(x_n)+K(x_n, X)\\,K(X,X)^{-1}\\,\\bigl(Y - m(X)\\bigr)\\\\[6pt]\n",
        "\\sigma^2_{XY}(x_n, X)\n",
        "&= K(x_n, x_n)\\;-\\;K(x_n, X)\\,K(X,X)^{-1}\\,K(X, x_n) \\\\[8pt]\n",
        "K(x_n, X)\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "k\\bigl(x_n, x^{(1)}\\bigr) & \\cdots & k\\bigl(x_n, x^{(n_d)}\\bigr)\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Here, the conditioned mean is dependant on a mean function. We will explore how the mean function influences the GP regressor later. For now, this is set to a zero matrix. With this, the mean vector simplifies to the following.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mu_{XY}(x_n)\n",
        "&= K(x_n, X)\\,K(X,X)^{-1}\\,Y\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We can also include a noise term. This is useful when there is noise during sampling. The nosie term is a hyperparameter that is learnt. (Although the implimentation of GP here accoutns for the noise, the inference does not take it into account such that visualisation of the GP output is easier.)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mu_{XY}(x_n)\n",
        "&= K(x_n, X)\\,[K(X,X)+{\\sigma_n^2}I]^{-1}\\,Y\\\\\\\\[6pt]\n",
        "\\sigma^2_{XY}(x_n, X)\n",
        "&= K(x_n, x_n)\\;-\\;K(x_n, X)\\,[K(X,X)+{\\sigma_n^2}I]^{-1}\\,K(X, x_n) \\\\[8pt]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "For this excersise, observe the function GP_inference_np within the GP_model_zeromean class and see how the equations are implimented to obtain the mean and variance of every point in the interested region of the x input space.\n",
        "\n",
        "We will now obtain the GP model of the input space with $x$ between -5 to 5 conditioned on the training points listed earlier. Observe how the input space (test data) is prepared and used to obtain the mean and variance via the GP_inference_np attribute.\n",
        "\n",
        "``` python\n",
        "# define the interested input space (test data)\n",
        "n_test      = 500\n",
        "Xtest       = np.linspace(-5, 5, num=n_test)\n",
        "\n",
        "# create the correct sized zero matrices to store the mean and standard deviation\n",
        "Ytest_mean  = np.zeros(n_test)\n",
        "Ytest_std   = np.zeros(n_test)\n",
        "\n",
        "# use GP to predict test data by calling on the GP_inference_np attribute\n",
        "for ii in range(n_test):\n",
        "    m_ii, std_ii   = GP_m.GP_inference_np(Xtest[ii])\n",
        "    Ytest_mean[ii] = m_ii\n",
        "    Ytest_std[ii]  = std_ii\n",
        "```\n",
        "\n",
        "We can then plot the mean and standard deviation to show the outcome of the GP model! After plotting, observe how the confidence interval (here, is set to +- 3 standard deviations) changes as we move further away from the training data points? What do we observe this behavior? Link this back to the kernel function and the covariance matrix!\n",
        "\n",
        "``` python\n",
        "ax = plt.figure(figsize = (8,4), dpi = 100)\n",
        "\n",
        "# plot observed points\n",
        "plt.plot(Xtrain, Ytrain, 'kx', mew=2)\n",
        "\n",
        "# plot the objective function to see compare the GP model to\n",
        "fx_test     = np.sin(Xtest)\n",
        "plt.plot(Xtest, fx_test, 'black', linewidth=1)\n",
        "\n",
        "# plot GP mean\n",
        "plt.plot(Xtest, Ytest_mean, 'C0', lw=2)\n",
        "\n",
        "# plot GP confidence intervals - here it is set to 3 standard deviations\n",
        "plt.gca().fill_between(Xtest.flat,\n",
        "                       Ytest_mean - 3*np.sqrt(Ytest_std),\n",
        "                       Ytest_mean + 3*np.sqrt(Ytest_std),\n",
        "                       color='C0', alpha=0.2)\n",
        "\n",
        "plt.title('Gaussian Process Regression')\n",
        "plt.legend(('Training Data', 'Objective Function', 'GP mean', 'GP Confidence Interval'),\n",
        "           loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc095d2-2257-48dd-9968-44949d415b1c",
      "metadata": {
        "id": "ecc095d2-2257-48dd-9968-44949d415b1c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dc3c7cb8-9031-4b22-a141-bc5a9dffbe3a",
      "metadata": {
        "id": "dc3c7cb8-9031-4b22-a141-bc5a9dffbe3a"
      },
      "source": [
        "#### Excersise B1_5:\n",
        "##### Concepts: GP regession\n",
        "\n",
        "It is your turn to perform GP regression! Show a plot similar to that generated in the previous excersise of a GP regression conditioned on the training data and objective function specified.\n",
        "\n",
        "``` python\n",
        "# x training data set based on [0.1,0.3,0.4,0.8,2,2.1]\n",
        "# y training data set based objective function sin(x^x)\n",
        "# interested x input space from 0 to 3\n",
        "```\n",
        "\n",
        "Is the GP regressor better for extrapolation beyond the training data set? What values obtained from the regressor can allow us to make a judgement on the confidence of the predicted mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb6280f-929d-4e79-8800-64e2ea7adeef",
      "metadata": {
        "id": "aeb6280f-929d-4e79-8800-64e2ea7adeef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cdc86493-6ec2-41ad-875b-7098ee269122",
      "metadata": {
        "id": "cdc86493-6ec2-41ad-875b-7098ee269122"
      },
      "source": [
        "#### Excersise B1_6:\n",
        "##### Concepts: Kernel function Hyperparameters\n",
        "\n",
        "Well done! You have now sucessfully performed a GP regression!\n",
        "\n",
        "In this section, we will see how the hyperparameters influence the kernel functions similar to how we have shown it in excersise B1_1.\n",
        "\n",
        "The hyperparameters are 'learnt' by minimizing the negative log likelihood.\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\Theta)\n",
        "\\;=\\;\n",
        "-\\log p(Y\\mid X)\n",
        "\\;=\\;\n",
        "\\frac{1}{2}\\,Y^T\\,K(\\Theta)^{-1}\\,Y\n",
        "\\;+\\;\n",
        "\\frac{1}{2}\\,\\log\\!\\bigl(\\det K(\\Theta)\\bigr)\n",
        "$$\n",
        "\n",
        "This equation is derived from the probability of the model to reproduce the training data. Observe how in the GP_model_meanzero class the negative loglikelihood is optimised via scipy.optimize.minimize() function. The optimise hyperparameters is what enables the kernel functions and therefore the covariance matrix to be built.\n",
        "\n",
        "In this implimentation of GP, there are N + 2 number of hyperparameters where N is the total number of dimensions. This involve the signal variance, noise variance and the individual lengthscales for each dimension. From the GP regression you performed, obtain the values of the optimised hyperparameters. These hyperparameters are instance attributes of the object. See excersise B1_3 for hints for obtaining the values.\n",
        "\n",
        "How would these values change for different objective functions? How can we show this with the code already provided?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50fa6e02-ba21-4464-b6a9-857c1f384e15",
      "metadata": {
        "id": "50fa6e02-ba21-4464-b6a9-857c1f384e15"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fa03e821-c4f0-4553-a42c-e8fc900fec44",
      "metadata": {
        "id": "fa03e821-c4f0-4553-a42c-e8fc900fec44"
      },
      "source": [
        "#### Excersise B1_6:\n",
        "##### Concepts: The mean function\n",
        "\n",
        "In this final section, we will observe how the mean function alters the GP regressor. So far, we have set the GP mean function to 0. What would change if the mean function was altered?\n",
        "\n",
        "Here, the GP_model_meanzero class has been moidified to allow for the mean function to be specified. What are the changes?\n",
        "\n",
        "```python\n",
        "class GP_model_variablemean:\n",
        "\n",
        "    def __init__(self, X, Y, kernel, hyperparams_multistart_loops):\n",
        "        self.X, self.Y, self.kernel = X, Y, kernel\n",
        "        self.number_of_point, self.nx_dimensions, self.ny_dimensions = X.shape[0], X.shape[1], Y.shape[1]\n",
        "        self.multistart_loops            = hyperparams_multistart_loops\n",
        "        self.X_mean, self.X_std     = np.mean(X, axis=0), np.std(X, axis=0)\n",
        "        self.Y_mean, self.Y_std     = np.mean(Y, axis=0), np.std(Y, axis=0)\n",
        "        self.X_norm, self.Y_norm    = (X-self.X_mean)/self.X_std, (Y-self.Y_mean)/self.Y_std\n",
        "        self.hyperparam_optimized , self.inverse_covariance_matrix_opt   = self.determine_hyperparameters()     \n",
        "        \n",
        "    def Cov_mat(self, kernel, X_norm, W, sf2):\n",
        "        if kernel == 'SquaredExponential':\n",
        "            xixj_euclidean_distance = cdist(X_norm, X_norm, 'seuclidean', V=W)**2\n",
        "            cov_matrix = sf2*np.exp(-0.5*xixj_euclidean_distance)\n",
        "            return (cov_matrix)\n",
        "        else:\n",
        "            print('ERROR no kernel with name ', kernel)\n",
        "            \n",
        "    def negative_loglikelihood(self, hyper, X, Y):\n",
        "        n_point, nx_dim = self.number_of_point, self.nx_dimensions\n",
        "        kernel          = self.kernel\n",
        "        W               = np.exp(2*hyper[:nx_dim])   \n",
        "        sf2             = np.exp(2*hyper[nx_dim])   \n",
        "        sn2             = np.exp(2*hyper[nx_dim+1])  \n",
        "        K       = self.Cov_mat(kernel, X, W, sf2)\n",
        "        K       = K + (sn2 + 1e-8)*np.eye(n_point)\n",
        "        K       = (K + K.T)*0.5                    \n",
        "        L       = np.linalg.cholesky(K)           \n",
        "        logdetK = 2 * np.sum(np.log(np.diag(L)))   \n",
        "        invLY   = np.linalg.solve(L,Y)             \n",
        "        alpha   = np.linalg.solve(L.T,invLY)      \n",
        "        NLL     = np.dot(Y.T,alpha) + logdetK      \n",
        "        return (NLL)\n",
        "        \n",
        "    def determine_hyperparameters(self):\n",
        "        lower_bound = np.array([-4.]*(self.nx_dimensions+1) + [-8.])\n",
        "        upper_bound = np.array([4.]*(self.nx_dimensions+1) + [ -2.])\n",
        "        bounds = np.hstack((lower_bound.reshape(self.nx_dimensions+2,1), upper_bound.reshape(self.nx_dimensions+2,1)))\n",
        "        multi_startvec   = sobol_seq.i4_sobol_generate(self.nx_dimensions + 2, self.multistart_loops)\n",
        "        temp_min_hyperparams = [0.]*self.multistart_loops\n",
        "        temp_loglikelihood = np.zeros((self.multistart_loops))\n",
        "        hyperparam_optimized = np.zeros((self.nx_dimensions+2, self.ny_dimensions)) #for best solutions\n",
        "        inverse_covariance_matrix_opt = []\n",
        "        for i in range(self.ny_dimensions):\n",
        "            for j in range(self.multistart_loops ):\n",
        "                hyperparams_initialisation   = lower_bound + (upper_bound-lower_bound)*multi_startvec[j,:]\n",
        "                result = minimize(self.negative_loglikelihood,\n",
        "                               hyperparams_initialisation,\n",
        "                               args=(self.X_norm, self.Y_norm[:,i]),\n",
        "                               method='SLSQP',\n",
        "                               options={'disp':False,'maxiter':10000},\n",
        "                               bounds=bounds,\n",
        "                               tol=1e-12)\n",
        "                temp_min_hyperparams[j] = result.x\n",
        "                temp_loglikelihood[j] = result.fun  \n",
        "            minimumloglikelihood_index    = np.argmin(temp_loglikelihood)\n",
        "            hyperparam_optimized[:,i] = temp_min_hyperparams[minimumloglikelihood_index  ]\n",
        "            lengthscale_opt      = np.exp(2.*hyperparam_optimized[:self.nx_dimensions,i])\n",
        "            signalvarience_opt      = np.exp(2.*hyperparam_optimized[self.nx_dimensions,i])\n",
        "            noise_opt      = np.exp(2.*hyperparam_optimized[self.nx_dimensions+1,i]) + 1e-8\n",
        "            covarience_matrix_opt = self.Cov_mat(self.kernel, self.X_norm, lengthscale_opt,signalvarience_opt) + noise_opt*np.eye(self.number_of_point)\n",
        "            inverse_covariance_matrix_opt     += [np.linalg.solve(covarience_matrix_opt, np.eye(self.number_of_point))]\n",
        "        return (hyperparam_optimized , inverse_covariance_matrix_opt)\n",
        "\n",
        "    def calc_cov_sample(self,xnorm,Xnorm,ell,sf2):\n",
        "        nx_dim = self.nx_dimensions\n",
        "        dist = cdist(Xnorm, xnorm.reshape(1,nx_dim), 'seuclidean', V=ell)**2\n",
        "        cov_matrix = sf2 * np.exp(-.5*dist)\n",
        "        return (cov_matrix )         \n",
        "\n",
        "    def GP_inference_np(self, x, mean_function):\n",
        "        nx_dim                   = self.nx_dimensions\n",
        "        kernel, ny_dim           = self.kernel, self.ny_dimensions\n",
        "        hypopt, Cov_mat          = self.hyperparam_optimized, self.Cov_mat\n",
        "        stdX, stdY, meanX, meanY = self.X_std, self.Y_std, self.X_mean, self.Y_mean\n",
        "        calc_cov_sample          = self.calc_cov_sample\n",
        "        invKsample               = self.inverse_covariance_matrix_opt\n",
        "        Xsample, Ysample         = self.X_norm, self.Y_norm\n",
        "        xnorm = (x - meanX)/stdX\n",
        "        mean  = np.zeros(ny_dim)\n",
        "        var   = np.zeros(ny_dim)\n",
        "        \n",
        "        for i in range(ny_dim):\n",
        "            invK           = invKsample[i]\n",
        "            hyper          = hypopt[:,i]\n",
        "            ellopt, sf2opt = np.exp(2*hyper[:nx_dim]), np.exp(2*hyper[nx_dim])\n",
        "            k       = calc_cov_sample(xnorm,Xsample,ellopt,sf2opt)\n",
        "            mean[i] = mean_function + np.matmul(np.matmul(k.T,invK),(Ysample[:,i]-mean_function))\n",
        "            var[i]  = max(0, sf2opt - np.matmul(np.matmul(k.T,invK),k))\n",
        "\n",
        "        mean_sample = mean*stdY + meanY\n",
        "        var_sample  = var*stdY**2\n",
        "        return (mean_sample, var_sample)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a2bfc4-3eba-486c-938d-6953b43b7739",
      "metadata": {
        "id": "e8a2bfc4-3eba-486c-938d-6953b43b7739"
      },
      "source": [
        "#### Excersise B1_7:\n",
        "##### Concepts: The mean function\n",
        "\n",
        "Notice that the mean function that is implimented here is a N by M matrix where N is the number of points and M is the dimension of the inputs. We can now run a modified version of excersise B1_4 with a non-zero mean function. Please execute the code for GP_model_variablemean class.\n",
        "\n",
        "We will plot the GP outputs with different mean functions. These are set to a constant value f(x) = C. (One can also modify the mean function to be any function such as a polynomial). What is the effect of modifying the mean function? Hint: observe areas where it is further away from the training points. What do you notice?\n",
        "\n",
        "Given in usual senarios that we do not understand the relationship between input and outputs, what is a suitable mean function to use?\n",
        "\n",
        "``` python\n",
        "mean_func_list = [1,5,10,20]\n",
        "\n",
        "for constant in mean_func_list:\n",
        "    # define and reshape training data\n",
        "    Xtrain = np.array([-4,-1.5,-1,0])\n",
        "    ndata  = Xtrain.shape[0]\n",
        "    Xtrain = Xtrain.reshape(ndata,1)\n",
        "    Ytrain = np.sin(Xtrain)\n",
        "    \n",
        "    # build the GP object (instance of the DP_model_meanzero class)\n",
        "    GP_m = GP_model_variablemean(Xtrain, Ytrain, 'SquaredExponential',  hyperparams_multistart_loops=3)\n",
        "    \n",
        "    # define the interested input space (test data)\n",
        "    n_test      = 500\n",
        "    Xtest       = np.linspace(-5.0, 5.0, num=n_test)\n",
        "    \n",
        "    # create the correct sized zero matrices to store the mean and standard deviation\n",
        "    Ytest_mean  = np.zeros(n_test)\n",
        "    Ytest_std   = np.zeros(n_test)\n",
        "    \n",
        "    # create the mean function\n",
        "    mean_func = lambda x: constant\n",
        "    \n",
        "    # use GP to predict test data by calling on the GP_inference_np attribute\n",
        "    for ii in range(n_test):\n",
        "        m_ii, std_ii   = GP_m.GP_inference_np(Xtest[ii], mean_func(Xtest[ii]))\n",
        "        Ytest_mean[ii] = m_ii\n",
        "        Ytest_std[ii]  = std_ii\n",
        "    \n",
        "    ax = plt.figure(figsize = (8,4), dpi = 100)\n",
        "    \n",
        "    # plot observed points\n",
        "    plt.plot(Xtrain, Ytrain, 'kx', mew=2)\n",
        "    \n",
        "    # plot the objective function to see compare the GP model to\n",
        "    fx_test     = np.sin(Xtest)\n",
        "    plt.plot(Xtest, fx_test, 'black', linewidth=1)\n",
        "    \n",
        "    # plot GP mean\n",
        "    plt.plot(Xtest, Ytest_mean, 'C0', lw=2)\n",
        "    \n",
        "    # plot GP confidence intervals - here it is set to 3 standard deviations\n",
        "    plt.gca().fill_between(Xtest.flat,\n",
        "                           Ytest_mean - 3*np.sqrt(Ytest_std),\n",
        "                           Ytest_mean + 3*np.sqrt(Ytest_std),\n",
        "                           color='C0', alpha=0.2)\n",
        "    \n",
        "    plt.title(f'Gaussian Process Regression - Mean function f(x) = {constant}')\n",
        "    plt.legend(('Training Data', 'Objective Function', 'GP mean', 'GP Confidence Interval'),\n",
        "               loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de666c3-fc40-4b63-9bbb-b3d36e375605",
      "metadata": {
        "id": "3de666c3-fc40-4b63-9bbb-b3d36e375605"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1a219a29-607d-46ae-bdcd-785b38af9c2b",
      "metadata": {
        "id": "1a219a29-607d-46ae-bdcd-785b38af9c2b"
      },
      "source": [
        "#### Excersise B1_8:\n",
        "\n",
        "Final excersise! We will now observe how to perform multidimensional GP regression by observing how to construct the inputs.\n",
        "\n",
        "The GP_model_meanzero class is built to recieve multidimensional matrices of shape N x M where N is the number of poinrs and M is the number of dimensions as inputs. One can contruct these multidimensional arrays mannually, or use a recursively.\n",
        "\n",
        "```python\n",
        "Xtrain = array([[-4, -3],\n",
        "               [-4, -2],\n",
        "               [-4,  3],\n",
        "               [-1, -3],\n",
        "               [-1, -2],\n",
        "               [-1,  3],\n",
        "               [ 2, -3],\n",
        "               [ 2, -2],\n",
        "               [ 2,  3]])\n",
        "\n",
        "# the same as\n",
        "\n",
        "x1loc  = [-4,  -1,  2]; x2loc = [-3, -2, 3]\n",
        "Xtrain = [[x,y] for x in x1loc for y in x2loc]\n",
        "X_training = np.array(Xtrain)\n",
        "```\n",
        "\n",
        "In this set of excersises, we are only with the optimisation of a one dimensional output. Here is output must be of shape N x M where N is the number of points and M = 1 (for the number of dimension). Example:\n",
        "\n",
        "```python\n",
        "#an objective function defined to take in individual points as a list\n",
        "def obj_func(X):\n",
        "\treturn (np.sin(X[0])+np.sin(X[1]))\n",
        "\n",
        "#obtaining the function and reshaping to obtain N x 1 matrix\n",
        "Fx_training     = np.array([obj_func(x) for x in X_training])\n",
        "Y_training  = Fx_training.reshape(Fx_training.shape[0],1)\n",
        "```\n",
        "\n",
        "In the same fashion, we must modify our interested input space and empty matrix to have the correct 2 dimensional shape.\n",
        "\n",
        "``` python\n",
        "number_points_pervariable      = 50\n",
        "number_points_searchspace   = number_points_pervariable ** (np.shape(X_training)[1])\n",
        "\n",
        "#Here, the interested search space is symmetrical, if it is non symmetrical, one can define each dimension individually and build the search space recursively\n",
        "X_searchspace     = np.linspace(-5, 8, num=number_points_pervariable)\n",
        "X_searchspace     = np.array([[x,y] for x in X_searchspace for y in X_searchspace])\n",
        "\n",
        "Ysearchspace_mean        = np.zeros(number_points_searchspace)\n",
        "Ysearchspace_std         = np.zeros(number_points_searchspace)\n",
        "```\n",
        "\n",
        "Using the objective function and the training points specified above, can you perform GP regression and obtain the covarience matrix? Can you graph a 3D plot of the predicted average mean with the described search space (compare it with the objective function)? (Standard deviation fill-in regions are difficult to visualise in 3D.)\n",
        "\n",
        "What is the predicted output mean and standard deviation of point [1,1]?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bdf8c00-6e7a-40af-a5a3-74313e2fe97d",
      "metadata": {
        "id": "4bdf8c00-6e7a-40af-a5a3-74313e2fe97d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2560080e-4507-4094-b5a0-5933d1e0a3e0",
      "metadata": {
        "id": "2560080e-4507-4094-b5a0-5933d1e0a3e0"
      },
      "source": [
        "### Segue into Bayesian Optimisation!\n",
        "\n",
        "Well done for getting to this point! You should now be familiar with concepts such as how the covariance is built from kernel functions, how the GP regressor provides predictions on mean with an associated uncertainty and how the mean function affects the predicted mean.\n",
        "\n",
        "Bayesian Optimisation is an active learning workflow that utilizes the mean and uncertainty obtained from GP regression to perform optimisation tasks. You should have managed to produce two beautiful graphs from the previous excersise describing the objective function and the predicted mean based on the training points.\n",
        "\n",
        "![download.png](attachment:29a466b6-ee57-4070-80d2-bd4621653940.png)\n",
        "![download.png](attachment:452a5b64-c955-4b8d-9ea1-3d5082f866d2.png)\n",
        "\n",
        "Before heading to section C, have a little think about how the predicted mean and uncertainty can help us find the minima of the objective function if we were able to evaluate additional points beyond that of the initial trianing data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}